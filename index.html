<!DOCTYPE HTML>
<html lang="en">

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <title>Osher Azulay</title>
  <meta name="author" content="Osher Azulay">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link rel="shortcut icon" href="images/favicon/favicon.ico" type="image/x-icon">
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
</head>

<body>
  <table
    style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto">
    <tbody>
      <tr style="padding:0px">
        <td style="padding:0px">

          <!-- Header -->
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto">
            <tbody>
              <tr style="padding:0px">
                <td style="padding:2.5%;width:63%;vertical-align:middle">
                  <p class="name" style="text-align:center">Osher Azulay</p>

                  <p>
                    I’m a <a href="https://www.fulbright.org.il/node/900">Fulbright</a> postdoctoral researcher at the
                    University of Michigan, working with <a href="https://web.eecs.umich.edu/~stellayu/">Prof. Stella
                      Yu</a>.
                    I work at the intersection of robotics, computer vision, tactile sensing, and machine learning,
                    aiming to advance humanoid intelligence.
                  </p>

                  <p>
                    Previously, I earned my Ph.D. from Tel Aviv University in 2024, under the supervision of
                    <a href="https://robotics.sites.tau.ac.il/">Dr. Avishai Sintov</a>. My work focused on robotic
                    in-hand manipulation, developing methods that leverage multimodal cues to enable more adaptive
                    interaction.
                  </p>

                  <p>Always happy to connect—feel free to reach out.</p>

                  <p style="text-align:center">
                    <a href="mailto:azulayosher@gmail.com">Email</a> &nbsp;/&nbsp;
                    <a href="data/Osher-CV.pdf">CV</a> &nbsp;/&nbsp;
                    <a href="https://scholar.google.com/citations?user=XQdRWyUAAAAJ&hl=iw">Scholar</a> &nbsp;/&nbsp;
                    <a href="https://www.linkedin.com/in/osher-azulay-20ab38154/">LinkedIn</a> &nbsp;/&nbsp;
                    <a href="https://github.com/osheraz">Github</a>
                  </p>
                </td>

                <td style="padding:2.5%;width:30%;max-width:30%">
                  <a href="images/osher_face.jpg">
                    <img style="width:100%;max-width:100%;object-fit:cover;border-radius:50%;" alt="profile photo"
                      src="images/osher_face.jpg" class="hoverZoomLink">
                  </a>
                </td>
              </tr>
            </tbody>
          </table>

          <!-- Main content -->
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>

              <!-- News -->
              <tr>
                <td colspan="2" style="padding:20px;">
                  <h2>News</h2>
                  <div class="news-container">
                    <ul>
                      <li>July 2025 — Started my postdoc at the University of Michigan.</li>
                      <li>Winter 2025 — Visiting Scholar at UC Berkeley’s AUTOLab.</li>
                      <li>April 2025 — Gave a talk at Columbia University’s ROAM Lab.</li>
                      <li>Dec 2024 — Invited talk at Bar-Ilan University, Computer Science Department.</li>
                      <li>Dec 2024 — Invited talk at the Technion, Mechanical Engineering Robotics Colloquium.</li>
                      <li>Nov 2024 — Received the Fulbright Postdoctoral Fellowship.</li>
                      <li>Oct 2024 — Defended my Ph.D. at Tel Aviv University.</li>
                      <li>Summer 2023 — Visiting Graduate Researcher at Rutgers University, Robot Learning Lab.</li>
                      <li>Summer 2022 — Robotics Intern Engineer at Unlimited Robotics.</li>
                      <li>2023 — Received Honorable Mention for Excellence in Teaching at Tel Aviv University.</li>
                      <li>2023 — Awarded the KLA Ph.D. Excellence Scholarship.</li>
                      <li>2022 — Awarded the Prof. Nehemia Levtzion Scholarship for Outstanding Doctoral Students.</li>
                    </ul>
                  </div>
                </td>
              </tr>

              <!-- Publications header -->
              <tr>
                <td colspan="2" style="padding:20px;">
                  <h2 style="text-align:left;">Selected Publications:</h2>
                </td>
              </tr>

              <!-- Pub 1 -->
              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="images/vigor.gif" alt="VIGOR" width="260" height="140">
                </td>
                <td width="75%" valign="middle">
                  <a href="https://vigor2026.github.io/">
                    <span class="papertitle">VIGOR: Visual Goal-In-Context Inference for Unified Humanoid Fall
                      Safety</span>
                  </a>
                  <br>
                  <strong><a href="https://osheraz.github.io/">Osher Azulay</a></strong>,
                  <a href="https://jerryxu0907.github.io/">Zhengjie Xu</a>,
                  <a href="https://schefferac2020.github.io/">Andrew Scheffer</a>,
                  and <a href="https://web.eecs.umich.edu/~stellayu/">Stella X. Yu</a>.
                  <br>
                  <em>Under review</em>.
                  <br>
                  <a href="https://vigor2026.github.io/">project page</a>
                  /
                  <a href="https://arxiv.org/abs/2602.16511">paper</a>
                  <p></p>
                  <p>Unified fall mitigation + stand-up recovery distilled into an egocentric-depth policy.</p>
                </td>
              </tr>

              <!-- Pub 2 -->
              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="images/viden.gif" alt="ViDEN" width="260" height="140">
                </td>
                <td width="75%" valign="middle">
                  <a href="https://nimicurtis.github.io/ViDEN/">
                    <span class="papertitle">Embodiment-Agnostic Navigation Policy Trained with Visual
                      Demonstrations</span>
                  </a>
                  <br>
                  <a href="https://nimicurtis.github.io/">Nimrod Curtis</a>*,
                  <a href="https://www.linkedin.com/in/osher-azulay">Osher Azulay</a>*,
                  and <a href="http://web2.eng.tau.ac.il/wtest/Avishailab/index.php/sintov/">Avishai Sintov</a>.
                  <br>
                  <em>Under review</em>.
                  <br>
                  <a href="https://nimicurtis.github.io/ViDEN/">project page</a>
                  /
                  <a href="https://arxiv.org/abs/2412.20226">paper</a>
                  /
                  <a href="https://github.com/nimiCurtis/pilot_bc">code</a>
                  /
                  <a href="https://www.youtube.com/watch?v=-iZPtkif8oI">video</a>
                  <p></p>
                  <p>Learns adaptive, collision-free motion from just a few visual demonstrations using diffusion.
                  </p>
                </td>
              </tr>

              <!-- Pub 3 -->
              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="images/test.gif" alt="Visuotactile insertion" width="260" height="140">
                </td>
                <td width="75%" valign="middle">
                  <a href="https://arxiv.org/abs/2411.06408">
                    <span class="papertitle">Visuotactile-Based Learning for Insertion with Compliant Hands</span>
                  </a>
                  <br>
                  <strong>Osher Azulay</strong>, <a href="https://dhruvmetha.github.io/">Dhruv Metha Ramesh</a>,
                  <a href="https://nimicurtis.github.io/">Nimrod Curtis</a> and
                  <a href="http://web2.eng.tau.ac.il/wtest/Avishailab/index.php/sintov/">Avishai Sintov</a>.
                  <br>
                  <em>IEEE RA-L &amp; IROS</em>, 2025.
                  <br>
                  <a href="https://osheraz.github.io/visuotactile/">project page</a>
                  /
                  <a href="https://arxiv.org/abs/2411.06408">paper</a>
                  /
                  <a href="https://github.com/osheraz/IsaacGymInsertion">code</a>
                  <p></p>
                  <p>Multimodal visuotactile policy for accurate pose estimation and sim-to-real insertion.</p>
                </td>
              </tr>

              <!-- Pub 4 -->
              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="images/allsight.gif" alt="AllSight" width="260" height="160">
                </td>
                <td width="75%" valign="middle">
                  <a href="https://arxiv.org/abs/2307.02928">
                    <span class="papertitle">AllSight: A Low-Cost and High-Resolution Round Tactile Sensor with
                      Zero-Shot Learning Capability</span>
                  </a>
                  <br>
                  <strong>Osher Azulay</strong>,
                  <a href="https://www.linkedin.com/in/nimrod-curtis/?originalSubdomain=il">Nimrod Curtis</a>, Rotem
                  Sokolovsky,
                  Guy Levitski, Daniel Slomovik, Guy Lilling and
                  <a href="http://web2.eng.tau.ac.il/wtest/Avishailab/index.php/sintov/">Avishai Sintov</a>.
                  <br>
                  <em>IEEE RA-L &amp; ICRA</em>, 2024.
                  <br>
                  <a href="https://arxiv.org/abs/2307.02928">paper</a>
                  /
                  <a href="https://github.com/osheraz/allsight">code</a>
                  /
                  <a href="https://www.youtube.com/watch?v=Cv90pMow7SI&amp;ab_channel=AvishaiSintov">video</a>
                  <p></p>
                  <p>Introducing <em>AllSight</em>, an optical tactile sensor with a round 3D structure designed for
                    robotic inhand manipulation tasks</p>
                </td>
              </tr>

              <!-- Pub 5 -->
              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="images/aug.png" alt="Tactile simulators" width="260" height="160">
                </td>
                <td width="75%" valign="middle">
                  <a href="https://arxiv.org/abs/2309.10409">
                    <span class="papertitle">Augmenting Tactile Simulators with Real-like and Zero-Shot
                      Capabilities</span>
                  </a>
                  <br>
                  <strong>Osher Azulay</strong>*,
                  <a href="https://www.linkedin.com/in/alon-mizrahi-4978a2238/">Alon Mizrahi</a>*,
                  <a href="https://www.linkedin.com/in/nimrod-curtis/?originalSubdomain=il">Nimrod Curtis</a>* and
                  <a href="http://web2.eng.tau.ac.il/wtest/Avishailab/index.php/sintov/">Avishai Sintov</a>.
                  <br>
                  <em>ICRA 2024</em>.
                  <br>
                  <a href="https://arxiv.org/abs/2309.10409">paper</a>
                  /
                  <a href="https://github.com/RobLab-Allsight/allsight_sim2real">code</a>
                  <p></p>
                  <p>Bridges the sim-to-real gap for 3D shaped high-resolution tactile sensing using generative
                    modeling.</p>
                </td>
              </tr>

              <!-- Pub 6 -->
              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="images/se3.gif" alt="SE(3) insertion" width="260" height="160">
                </td>
                <td width="75%" valign="middle">
                  <a href="https://ieeexplore.ieee.org/abstract/document/9963587">
                    <span class="papertitle">Haptic-Based and SE(3)-Aware Object Insertion Using Compliant Hands</span>
                  </a>
                  <br>
                  <strong>Osher Azulay</strong>, <a
                    href="https://www.linkedin.com/in/max-monastirsky/?originalSubdomain=il">Max Monastirsky</a> and
                  <a href="http://web2.eng.tau.ac.il/wtest/Avishailab/index.php/sintov/">Avishai Sintov</a>.
                  <br>
                  <em>IEEE RA-L &amp; ICRA</em>, 2023.
                  <br>
                  <a href="https://ieeexplore.ieee.org/abstract/document/9963587">paper</a>
                  /
                  <a href="https://www.youtube.com/watch?v=arA6Klb5KpM&amp;ab_channel=AvishaiSintov">video</a>
                  <p></p>
                  <p>Exploring complaint hands characteristics for object insertion using haptic-based residual RL.</p>
                </td>
              </tr>

              <!-- Pub 7 -->
              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="images/throw.png" alt="Throwing" width="260" height="160">
                </td>
                <td width="75%" valign="middle">
                  <a href="https://ieeexplore.ieee.org/document/9984828">
                    <span class="papertitle">Learning to Throw With a Handful of Samples Using Decision
                      Transformers</span>
                  </a>
                  <br>
                  <a href="https://www.linkedin.com/in/max-monastirsky/?originalSubdomain=il">Max Monastirsky</a>,
                  <strong>Osher Azulay</strong> and
                  <a href="http://web2.eng.tau.ac.il/wtest/Avishailab/index.php/sintov/">Avishai Sintov</a>.
                  <br>
                  <em>IEEE RA-L &amp; IROS</em>, 2023.
                  <br>
                  <a href="https://ieeexplore.ieee.org/document/9984828">paper</a>
                  /
                  <a href="https://www.youtube.com/watch?v=5_G6o_H3HeE&amp;ab_channel=AvishaiSintov">video</a>
                  <p></p>
                  <p>Exploring the use of Decision Transformers for throwing and their ability for sim2real policy
                    transfer.</p>
                </td>
              </tr>

              <!-- Pub 8 -->
              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="images/haptic.gif" alt="Haptic pose estimation" width="260" height="160">
                </td>
                <td width="75%" valign="middle">
                  <a href="https://arxiv.org/abs/2207.02843">
                    <span class="papertitle">Learning Haptic-based Object Pose Estimation for In-hand Manipulation
                      Control with Underactuated Robotic Hands</span>
                  </a>
                  <br>
                  <strong>Osher Azulay</strong>, <a href="https://www.linkedin.com/in/inbar-meir-12773a8a/">Inbar
                    Meir</a> and
                  <a href="http://web2.eng.tau.ac.il/wtest/Avishailab/index.php/sintov/">Avishai Sintov</a>.
                  <br>
                  <em>IEEE Transactions on Haptics</em>, 2022.
                  <br>
                  <a href="https://arxiv.org/abs/2207.02843">paper</a>
                  /
                  <a href="https://github.com/osheraz/haptic_pose_estimation">code</a>
                  /
                  <a href="https://www.youtube.com/watch?v=aDdgix1BU5o&amp;ab_channel=AvishaiSintov">video</a>
                  <p></p>
                  <p>In-hand object pose estimation and manipulation using Model Predictive Control.</p>
                </td>
              </tr>

              <!-- Pub 9 -->
              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="images/open.gif" alt="Open-sourcing generative models" width="260" height="160">
                </td>
                <td width="75%" valign="middle">
                  <a href="https://neurips.cc/virtual/2021/workshop/38185">
                    <span class="papertitle">Open-Sourcing Generative Models for Data-driven Robot Simulations</span>
                  </a>
                  <br>
                  <a href="https://eranbamani.github.io/eranbamani/">Eran Bamani</a>, <strong>Osher Azulay</strong>,
                  Anton Gurevich, and <a href="http://web2.eng.tau.ac.il/wtest/Avishailab/index.php/sintov/">Avishai
                    Sintov</a>.
                  <br>
                  <em>Data-Centric AI workshop, NeurIPS</em>, 2021
                  <br>
                  <a href="https://neurips.cc/virtual/2021/workshop/38185">project page</a>
                  /
                  <a href="data/NeurIPS2021_Data_centric_AI.pdf">paper</a>
                  <p></p>
                  <p>Exploring the possibility of investing the recorded data in a generative model rather than directly
                    to a regression model for real-robot applications.</p>
                </td>
              </tr>

              <!-- Pub 10 -->
              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="images/komodo.gif" alt="Komodo" width="260" height="160">
                </td>
                <td width="75%" valign="middle">
                  <a href="https://ieeexplore.ieee.org/abstract/document/9344588/">
                    <span class="papertitle">Wheel Loader Scooping Controller Using Deep Reinforcement Learning</span>
                  </a>
                  <br>
                  <strong>Osher Azulay</strong> and
                  <a href="https://www.linkedin.com/in/amir-shapiro-b935a63/?originalSubdomain=il">Amir Shapiro</a>.
                  <br>
                  <em>IEEE Access</em>, 2021
                  <br>
                  <a href="https://ieeexplore.ieee.org/abstract/document/9344588/">paper</a>
                  /
                  <a href="https://github.com/osheraz/komodo">code</a>
                  /
                  <a href="https://www.youtube.com/watch?v=pYX7Ppq_PY4&amp;ab_channel=OsherAzulay">video</a>
                  <p></p>
                  <p>A deep reinforcement learning-based controller for an unmanned ground vehicle with a custom-built
                    scooping mechanism.</p>
                </td>
              </tr>

              <!-- Footer -->
              <tr>
                <td colspan="2" style="padding:20px 20px 0 20px;">
                  <div style="text-align:right;">
                    <div style="display:inline-block;width:150px;height:95px;overflow:hidden;margin-bottom:4px;">
                      <script type="text/javascript" id="mapmyvisitors"
                        src="https://mapmyvisitors.com/map.js?cl=ffffff&w=a&t=tt&d=Bg7o1W8RJ-lrLQMpQ-VbEqj-u5qpF6inCSqTV91qmTc&co=2d78ad&cmo=3acc3a&cmn=ff5353&ct=ffffff">
                        </script>
                    </div>
                    <p style="font-size:small;margin:0;">
                      Template from <a href="https://github.com/jonbarron/jonbarron_website">source code</a>.
                    </p>
                  </div>
                </td>
              </tr>

            </tbody>
          </table>

        </td>
      </tr>
    </tbody>
  </table>

  <script>
    const toggleBtn = document.getElementById('toggle-news');
    const hiddenNews = document.querySelectorAll('.more-news');
    let expanded = false;

    if (toggleBtn) {
      toggleBtn.addEventListener('click', () => {
        expanded = !expanded;
        hiddenNews.forEach(item => {
          item.style.display = expanded ? 'list-item' : 'none';
        });
        toggleBtn.textContent = expanded ? 'Show less' : 'Show more';
      });
    }
  </script>
</body>

</html>