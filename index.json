[{"authors":null,"categories":null,"content":"I‚Äôm a passionate roboticist pursuing my PhD studies in Mechanical engineering. My interests include dynamic manipulations, intelligent decision making, and machine learning. Find my latest work here, feel free to contact me for any questions!\n  Download my resum√©.\n","date":1549324800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1567641600,"objectID":"a93a02c7d269b21a82211c877f428401","permalink":"","publishdate":"0001-01-01T00:00:00Z","relpermalink":"","section":"authors","summary":"I‚Äôm a passionate roboticist pursuing my PhD studies in Mechanical engineering. My interests include dynamic manipulations, intelligent decision making, and machine learning. Find my latest work here, feel free to contact me for any questions!","tags":null,"title":"Osher Azulay","type":"authors"},{"authors":null,"categories":null,"content":"   Table of Contents  What you will learn Program overview Courses in this program Meet your instructor FAQs    What you will learn  Fundamental Python programming skills Statistical concepts and how to apply them in practice Gain experience with the Scikit, including data visualization with Plotly and data wrangling with Pandas  Program overview The demand for skilled data science practitioners is rapidly growing. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi.\nCourses in this program  Python basics Build a foundation in Python.   Visualization Learn how to visualize data with Plotly.   Statistics Introduction to statistics for data science.   Meet your instructor admin FAQs Are there prerequisites? There are no prerequisites for the first course.\n How often do the courses run? Continuously, at your own pace.\n  Begin the course   ","date":1611446400,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1611446400,"objectID":"5581ad20068553b6eb989d500426360d","permalink":"https://osheraz.github.io/blog-unused/example/","publishdate":"2021-01-24T00:00:00Z","relpermalink":"/blog-unused/example/","section":"blog-unused","summary":"An example of using Wowchemy's Book layout for publishing online courses.","tags":null,"title":"üìä Learn Data Science","type":"book"},{"authors":null,"categories":null,"content":"Build a foundation in Python.\n  1-2 hours per week, for 8 weeks\nLearn   Quiz What is the difference between lists and tuples? Lists\n Lists are mutable - they can be changed Slower than tuples Syntax: a_list = [1, 2.0, 'Hello world']  Tuples\n Tuples are immutable - they can\u0026rsquo;t be changed Tuples are faster than lists Syntax: a_tuple = (1, 2.0, 'Hello world')   Is Python case-sensitive? Yes\n","date":1609459200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609459200,"objectID":"e5ef6bc202b8cc67c96769a1d5d3e9d5","permalink":"https://osheraz.github.io/blog-unused/example/python/","publishdate":"2021-01-01T00:00:00Z","relpermalink":"/blog-unused/example/python/","section":"blog-unused","summary":"Build a foundation in Python.\n","tags":null,"title":"Python basics","type":"book"},{"authors":null,"categories":null,"content":"Learn how to visualize data with Plotly.\n  1-2 hours per week, for 8 weeks\nLearn   Quiz When is a heatmap useful? Lorem ipsum dolor sit amet, consectetur adipiscing elit.\n Write Plotly code to render a bar chart import plotly.express as px data_canada = px.data.gapminder().query(\u0026quot;country == 'Canada'\u0026quot;) fig = px.bar(data_canada, x='year', y='pop') fig.show()  ","date":1609459200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609459200,"objectID":"26cc7d8400d234f80d8bc1588446bc94","permalink":"https://osheraz.github.io/blog-unused/example/visualization/","publishdate":"2021-01-01T00:00:00Z","relpermalink":"/blog-unused/example/visualization/","section":"blog-unused","summary":"Learn how to visualize data with Plotly.\n","tags":null,"title":"Visualization","type":"book"},{"authors":null,"categories":null,"content":"Introduction to statistics for data science.\n  1-2 hours per week, for 8 weeks\nLearn The general form of the normal probability density function is:\n$$ f(x) = \\frac{1}{\\sigma \\sqrt{2\\pi} } e^{-\\frac{1}{2}\\left(\\frac{x-\\mu}{\\sigma}\\right)^2} $$\n The parameter $\\mu$ is the mean or expectation of the distribution. $\\sigma$ is its standard deviation. The variance of the distribution is $\\sigma^{2}$.   Quiz What is the parameter $\\mu$? The parameter $\\mu$ is the mean or expectation of the distribution.\n","date":1609459200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609459200,"objectID":"350ab1e67a043cec75c14afea352d6cb","permalink":"https://osheraz.github.io/blog-unused/example/stats/","publishdate":"2021-01-01T00:00:00Z","relpermalink":"/blog-unused/example/stats/","section":"blog-unused","summary":"Introduction to statistics for data science.\n","tags":null,"title":"Statistics","type":"book"},{"authors":["Osher Azulay and Amir Shapiro"],"categories":null,"content":"","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577836800,"objectID":"74ea0c9671304ea17bb594081571cadb","permalink":"https://osheraz.github.io/publication/ddpg/","publishdate":"2020-01-01T00:00:00Z","relpermalink":"/publication/ddpg/","section":"publication","summary":"Robotics may still seem to some like a far-fetched fantasy, but the usage of robots for variety of industry fields, especially in the construction and agricultural domains, yield many advantages, such as economic efficiency, safety and availability. As some tasks are more complex than others, they often require extensive engineering experience and tedious manual tuning beyond the control algorithm itself. Reinforcement learning (RL) is a type of machine learning technique that holds the promise of enabling robots to learn large repertoires of behavioral skills with minimal human intervention using trial and error methods. However, practical real-world applications of reinforcement learning are relatively rare, as they often require unrealistic learning time, high sample complexity, and discrepancies between simulated and real physical system behavior (i.e the reality gap). To create robust deep RL controllers (DRL), that can achieve success in real-life scenarios, some techniques can be used such as learning from both domains and performing further system identification.\nIn the proposed research we presents a deep reinforcement learning-based controller for an unmanned ground vehicle with a custom-built scooping mechanism. The robot's aim is to autonomously perform earth scooping cycles with three degrees of freedom lift, tilt and the robot's velocity. While the majority of previous research to automate scooping processes are based on data recorded by expert operators, we present a method to autonomously control a wheel loader to perform scooping cycle using deep reinforcement learning methods without any user-provided demonstrations. The controller‚Äôs learning approach is based on the actor-critic, Deep Deterministic Policy Gradient algorithm which we use to map online sensor data as input to continuously update actuator commands. The training of the scooping policy network is done solely in a simplified simulation environment using a virtual physics engine, which converged to an average of 65% fill factor from full bucket capacity and 5 sec average cycle time.","tags":[],"title":"Wheel Loader Scooping Controller Using Deep Reinforcement Learning","type":"publication"},{"authors":["admin"],"categories":[],"content":"from IPython.core.display import Image Image('https://www.python.org/static/community_logos/python-logo-master-v3-TM-flattened.png')  print(\u0026quot;Welcome to Academic!\u0026quot;)  Welcome to Academic!  Install Python and JupyterLab Install Anaconda which includes Python 3 and JupyterLab.\nAlternatively, install JupyterLab with pip3 install jupyterlab.\nCreate or upload a Jupyter notebook Run the following commands in your Terminal, substituting \u0026lt;MY-WEBSITE-FOLDER\u0026gt; and \u0026lt;SHORT-POST-TITLE\u0026gt; with the file path to your Academic website folder and a short title for your blog post (use hyphens instead of spaces), respectively:\nmkdir -p \u0026lt;MY-WEBSITE-FOLDER\u0026gt;/content/post/\u0026lt;SHORT-POST-TITLE\u0026gt;/ cd \u0026lt;MY-WEBSITE-FOLDER\u0026gt;/content/post/\u0026lt;SHORT-POST-TITLE\u0026gt;/ jupyter lab index.ipynb  The jupyter command above will launch the JupyterLab editor, allowing us to add Academic metadata and write the content.\nEdit your post metadata The first cell of your Jupter notebook will contain your post metadata (front matter).\nIn Jupter, choose Markdown as the type of the first cell and wrap your Academic metadata in three dashes, indicating that it is YAML front matter:\n--- title: My post's title date: 2019-09-01 # Put any other Academic metadata here... ---  Edit the metadata of your post, using the documentation as a guide to the available options.\nTo set a featured image, place an image named featured into your post\u0026rsquo;s folder.\nFor other tips, such as using math, see the guide on writing content with Academic.\nConvert notebook to Markdown jupyter nbconvert index.ipynb --to markdown --NbConvertApp.output_files_dir=.  Example This post was created with Jupyter. The orginal files can be found at https://github.com/gcushen/hugo-academic/tree/master/exampleSite/content/post/jupyter\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1567641600,"objectID":"96dbaee51dcf392c3e91d0de5c005233","permalink":"https://osheraz.github.io/blog-unused/jupyter/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/blog-unused/jupyter/","section":"blog-unused","summary":"Learn how to blog in Academic using Jupyter notebooks","tags":[],"title":"Display Jupyter Notebooks with Academic","type":"blog-unused"},{"authors":["Osher Azulay"],"categories":[],"content":"\nWell, I took a really nice course last semester, which was all about autonomous driving. I really liked it because it was a hands-on course, with some of the nicest projects. The course introduced some of the core functions of autonomous driving system, such as localization and mapping, spatial perception and route planning which later on focus on deep learning solutions.\nThroughout the course, I thought it would be great to write a brief description about the projects I have done, for later projects I might do.\nProject No.1 TODO: add the github folder here\nTopics  Mapping and Localization Probabilistic Occupancy Grid Iterative Closet Points (ICP)  Modeling and understanding the environment is a crucial task for autonomous robotics, in particular for mobile robots. Occupancy Grid Mapping refers to a family of computer algorithms in probabilistic robotics for mobile robots. This address the problem of generating maps from noisy and uncertain sensor measurement data, with the assumption that the robot pose is known. The basic idea of the occupancy grid is to represent a map of the environment as an evenly spaced field of binary random variables each representing the presence of an obstacle at that location in the environment. In this project, we experienced with real captured driving data - KITTI Dataset. The data-set contains 6 hours of traffic scenarios at 10‚Äì100 Hz using a variety of sensor modalities such as high-resolution color and grayscale stereo cameras, a Velodyne 3D laser scanner, and a high-precision GPS/IMU inertial navigation system.\nThe goal of an occupancy mapping algorithm is to estimate the posterior probability over maps given the data: $p(m\\mid z_{1:t},x_{1:t})$, where $m$ is the map, $z_{1:t}$ is the set of measurements from time 1 to t, and $x_{1:t}$ is the set of robot poses from time 1 to t. If we let $m_{i}$ denote the grid cell with index $i$, then the notation $p(m_{i})$ represents the probability that cell i is occupied. The standard approach, then, is to break the problem down into smaller problems of estimating $p(m_{i}\\mid z_{{1:t}},x_{{1:t}})$ for all grid cells $m_{i}$. Each of these estimation problems is then a binary problem. This breakdown is convenient but does lose some of the structure of the problem, since it does not enable modeling dependencies between neighboring cells. Instead, the posterior of a map is approximated by factoring it into $p(m\\mid z_{{1:t}},x_{{1:t}})=\\prod _{i}p(m_{i}\\mid z_{{1:t}},x_{{1:t}})$.\nHmm, I think it\u0026rsquo;s enough intro, let\u0026rsquo;s dig into the implementation. For that I used the pykitti python package, which is a light package of minimal set of tools for working the the KITTI dataset. Basically, after passing the path of the recording, it provides each of the sensors data via generators for easy sequential access. This package assumes that you have already downloaded the calibration data associated with the sequences you want to work on, and that the directory structure is unchanged from the original structure laid out in the KITTI zip files.\nIn the GitHub folder above, you can find the implementation of the OccupancyMap class, which create and update the occupancy \u0026amp; threshold maps. I implemented the update function using the inverse sensor model described in Chapter 9 of ‚ÄúProbabilistic robotics‚Äù by Sebastian Thrun etal [2]. In each of the update functions, I looped over the perceptual field of the point cloud with respect to the current car position and updated the cell value of the occupancy map using the log odd form of the Bayes filter: $l(m_{i}|z_{1:t},x_{1:t}) = l(m_{i}|z_{t},x_{t}) + l(m_{i}|z_{1:t‚àí1},x_{1:t‚àí1}) ‚àí l_{p}(m_{i})$\nThe whole procedure of the occupancy map creation:\n At the beginning, I load the dataset and extract the rigid SE(3) pose transformation using dataset.oxts.T_w_imu w.r.t the first measurement as the origin. Load the rigid transformation from the IMU to the velodyne, which is the constant transformation matrix I used to transform the current measured point cloud to the world coordinates at each iteration. For each of the car transformation matrix w.r.t to the origin:  Get the pose of the car Transform the PCL to world coordinate - loop over the current PCL and multiply each point with the current Lidar transformation matrix. Importance sampling - get only the cells with more than one measurement in the vertical direction. Update the occupancy and threshold maps. Visualize the scene map, the current point cloud the current occupancy map.     Project No.2 Todo.\n Project No.4 Todo.\n Project No.4 Todo.\n ","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1567641600,"objectID":"354b62ed5d4b81e9d1923ad64e193b83","permalink":"https://osheraz.github.io/blog/slam/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/blog/slam/","section":"blog","summary":" ","tags":[],"title":"Mapping, Perception and More","type":"blog"},{"authors":["Osher Azulay"],"categories":[],"content":"Image courtesy to TagSlam Throughout my research, I had to track the $SE(3)$ pose of various objects for several applications. In this post ill summarize the ROS-based implementation of some of the marker-based methods. Basically, most of the marker-based methods that ill cover only requires a USB-Cam, except the last one, which requires the whole OptiTrack motion captures system. We uses OptiTrack at the lab for for the finer applications.\n Most of the videos, figures and explanation are taken from the authors implementation and are not mine. I just gather them for future use.   In the following, ill use the notation $T^{1}_{0}$ to describe the homogeneous transformation matrix between coordinates frame 1 to 0. Detailed explanation about marker-based transformation can be seen Here\nAprilTag Markers The AprilTag come in several different families, depending on how many bits a tag can represent. Some tag families have large, coarse bit blocks (16h5), while others are fine and smaller (36h11- most common). For lower resolution camera, consider the 16h5 family, For better ill suggest the 36h11 and for circular object you should use the 21h7 family.\nA full detailed explanation how to generate tags is described:\n AprilTag-Generation Making and Using AprilTags  Measuring the tag Size When entering (ill explain it later) the size of your tag for pose estimation it is important to know what \u0026ldquo;size\u0026rdquo; is actually a measure of. The measurement that is called \u0026ldquo;size\u0026rdquo; is shown as \u0026ldquo;edge length\u0026rdquo; in the image below. The tag size should not be measured from the outside of the tag. The tag size is defined as the distance between the detection corners, or alternately, the length of the edge between the white border and the black border. The following illustration marks the detection corners with red Xs and the tag size with a red arrow for a tag from the 48h12Custom tag family.\nCoordinate system The coordinate system has the origin at the camera center. The z-axis points from the camera center out the camera lens. The x-axis is to the right in the image taken by the camera, and y is up.\napriltag_ros Installation Starting with a working ROS installation (Kinetic and Melodic are supported):\nexport ROS_DISTRO=melodic # Set this to your distro, e.g. kinetic or melodic source /opt/ros/$ROS_DISTRO/setup.bash # Source your ROS distro mkdir -p ~/catkin_ws/src # Make a new workspace cd ~/catkin_ws/src # Navigate to the source space git clone https://github.com/AprilRobotics/apriltag.git # Clone Apriltag library git clone https://github.com/AprilRobotics/apriltag_ros.git # Clone Apriltag ROS wrapper cd ~/catkin_ws # Navigate to the workspace rosdep install --from-paths src --ignore-src -r -y # Install any missing packages catkin build # Build all packages in the workspace (catkin_make_isolated will work also)  The package works as shown in the below figure. The following default input topics are subscribed to (which can be remapped based on your needs):\n /camera/image_rect: a sensor_msgs/Image topic which contains the image (e.g. a frame of a video stream coming from a camera). The image is assumed to be undistorted, i.e. produced by a pinhole camera. I recommend to use the image_proc_node, which is meant to sit between the camera driver and vision processing nodes. image_proc removes camera distortion from the raw image stream, and if necessary will convert Bayer or YUV422 format image data to color. /camera/camera_info: a sensor_msgs/CameraInfo topic which contains the camera calibration matrix in /camera/camera_info/K. One can obtain a specific camera\u0026rsquo;s K via camera intrinsics calibration using any camera calibarion methods (Option 1, Option 2, Option 3 and so on..).  The Apriltag ROS package takes in a rectified camera feed and returns a list of detected tags and their 3D locations. In order for this to work the software needs to know what tags it is looking for and how large they are. These are defined in 2 config files: config/settings.yaml and config/tags.yaml.\nThe behavior of the ROS wrapper is fully defined by the two configuration files config/tags.yaml (which defines the tags and tag bundles to look for) and config/settings.yaml (which configures the core AprilTag 2 algorithm itself). Then, the following topics are output:\n /tf: relative pose between the camera frame and each detected tag\u0026rsquo;s or tag bundle\u0026rsquo;s frame (specified in tags.yaml) using tf. /tag_detections: the same information as provided by the /tf topic but as a custom message carrying the tag ID(s), size(s) and geometry_msgs/PoseWithCovarianceStamped pose information. /tag_detections_image: the same image as input by /camera/image_rect but with the detected tags highlighted.  settings.yaml parameters this file configures the detection algorithm parameters (most are self-explained)\ntag_family: 'tag36h11' # Tag family tag_border: 1 # Size (in bits) of the black border. Always 1 if made by optitag tag_threads: 2 # Number of detection thread. Tune per your CPU tag_decimate: 1.0 # Reduce the resolution of the image by this number. Increases speed at the sacrifice of detecting smaller tags tag_blur: 0.0 # tag_blur\u0026gt;0 blurs the image and tag_blur\u0026lt;0 sharpens the image tag_refine_edges: 1 # improves edge detection and therefore improves pose estimation. Light computation tag_refine_decode: 0 # reduces false negative detection rate. Medium computation tag_refine_pose: 0 # improves pose estimation accuracy. Heavy computation tag_debug: 0 # save debug images to ~/.ros Careful if running with video publish_tf: true # publish tag/bundle poses to /tf topic  tags.yaml parameters This file tells the algorithm what tags to look for in the environment and how large they are so they can be placed in 3D space. The software assumes distance measurements are in meters and orientation is in quaternions (One can use tf.transformation package for other transformations)\nstandalone_tags: [ {id: 10, size: 0.15}, {id: 20, size: 0.1}, {id: 30, size: 0.07} ] tag_bundles: [ { name: 'my_bundle', layout: [ {id: 0, size: 0.05, x: 0.0000, y: 0.0000, z: 0.0, qw: 1.0, qx: 0.0, qy: 0.0, qz: 0.0}, {id: 4, size: 0.05, x: 0.0548, y: -0.0522, z: 0.0, qw: 1.0, qx: 0.0, qy: 0.0, qz: 0.0}, {id: 3, size: 0.05, x: -0.0580, y: -0.0553, z: 0.0, qw: 1.0, qx: 0.0, qy: 0.0, qz: 0.0}, {id: 2, size: 0.05, x: 0.0543, y: 0.0603, z: 0.0, qw: 1.0, qx: 0.0, qy: 0.0, qz: 0.0}, {id: 1, size: 0.05, x: -0.0582, y: 0.0573, z: 0.0, qw: 1.0, qx: 0.0, qy: 0.0, qz: 0.0} ] } ]  As we can see in the above configuration, We can define 2 tracking methods, standalone_tags and tag_bundles. For standalone tags, i.e. each marker represents a unique object in the environment we provide an ID and size for each tag you want to detect. For tag_bundles is the new feature which let you track an object which represented by multiple tags to overcome occlusion and such. Upon detection of a single or multiple tags in the bundle, the software will report the 6 DOF pose of the bundle\u0026rsquo;s origin. When we create a bundle you specify a list of tags. Each tag has a tag ID, size, and 6 DOF location of the tag in reference to the bundle\u0026rsquo;s origin.\nImportant notes mentioned by the authors:\n No tag ID should appear twice with different sizes (this creates ambiguity in the detection) No tag ID should appear twice in the image (this creates ambiguity in the detection) It is fine for a tag with the same ID to be listed both in standalone_tags and in tag_bundles, as long as it has the same size.  A complete launch file which:\n Open the camera stream from the video_device with width and height parameters. Launch the image_proc node to remove distortions. Load the settings.yaml and tags.yaml configurations files to the parameters server. Run the apriltag_ros core node and publish the detected transformation.  \u0026lt;launch\u0026gt; \u0026lt;node name=\u0026quot;camera\u0026quot; pkg=\u0026quot;usb_cam\u0026quot; type=\u0026quot;usb_cam_node\u0026quot; output=\u0026quot;screen\u0026quot; \u0026gt; \u0026lt;param name=\u0026quot;video_device\u0026quot; value=\u0026quot;/dev/video4\u0026quot; /\u0026gt; \u0026lt;param name=\u0026quot;image_width\u0026quot; value=\u0026quot;1280\u0026quot; /\u0026gt; \u0026lt;param name=\u0026quot;image_height\u0026quot; value=\u0026quot;720\u0026quot; /\u0026gt; \u0026lt;param name=\u0026quot;pixel_format\u0026quot; value=\u0026quot;yuyv\u0026quot; /\u0026gt; \u0026lt;param name=\u0026quot;camera_frame_id\u0026quot; value=\u0026quot;rgb_cam_link\u0026quot; /\u0026gt; \u0026lt;/node\u0026gt; \u0026lt;node pkg=\u0026quot;image_proc\u0026quot; type=\u0026quot;image_proc\u0026quot; name=\u0026quot;iamge_proc_node\u0026quot; ns=\u0026quot;camera\u0026quot; /\u0026gt; \u0026lt;arg name=\u0026quot;launch_prefix\u0026quot; default=\u0026quot;\u0026quot; /\u0026gt; \u0026lt;!-- set to value=\u0026quot;gdbserver localhost:10000\u0026quot; for remote debugging --\u0026gt; \u0026lt;arg name=\u0026quot;node_namespace\u0026quot; default=\u0026quot;apriltag_ros_continuous_node\u0026quot; /\u0026gt; \u0026lt;arg name=\u0026quot;camera_name\u0026quot; default=\u0026quot;/camera\u0026quot; /\u0026gt; \u0026lt;arg name=\u0026quot;camera_frame\u0026quot; default=\u0026quot;camera\u0026quot; /\u0026gt; \u0026lt;arg name=\u0026quot;image_topic\u0026quot; default=\u0026quot;image_rect\u0026quot; /\u0026gt; \u0026lt;!-- Set parameters --\u0026gt; \u0026lt;rosparam command=\u0026quot;load\u0026quot; file=\u0026quot;$(find apriltag_ros)/config/settings.yaml\u0026quot; ns=\u0026quot;$(arg node_namespace)\u0026quot; /\u0026gt; \u0026lt;rosparam command=\u0026quot;load\u0026quot; file=\u0026quot;$(find apriltag_ros)/config/tags.yaml\u0026quot; ns=\u0026quot;$(arg node_namespace)\u0026quot; /\u0026gt; \u0026lt;node pkg=\u0026quot;apriltag_ros\u0026quot; type=\u0026quot;apriltag_ros_continuous_node\u0026quot; name=\u0026quot;$(arg node_namespace)\u0026quot; clear_params=\u0026quot;true\u0026quot; output=\u0026quot;screen\u0026quot; launch-prefix=\u0026quot;$(arg launch_prefix)\u0026quot; \u0026gt; \u0026lt;!-- Remap topics from those used in code to those on the ROS network --\u0026gt; \u0026lt;remap from=\u0026quot;image_rect\u0026quot; to=\u0026quot;$(arg camera_name)/$(arg image_topic)\u0026quot; /\u0026gt; \u0026lt;remap from=\u0026quot;camera_info\u0026quot; to=\u0026quot;$(arg camera_name)/camera_info\u0026quot; /\u0026gt; \u0026lt;param name=\u0026quot;camera_frame\u0026quot; type=\u0026quot;str\u0026quot; value=\u0026quot;$(arg camera_frame)\u0026quot; /\u0026gt; \u0026lt;param name=\u0026quot;publish_tag_detections_image\u0026quot; type=\u0026quot;bool\u0026quot; value=\u0026quot;true\u0026quot; /\u0026gt; \u0026lt;!-- default: false --\u0026gt; \u0026lt;/node\u0026gt; \u0026lt;/launch\u0026gt;   ArTag Markers In this part ill explain the usage of ar_track_alvar package for AR tag tracking. This package is a ROS wrapper for Alvar, an open source AR tag tracking library.\nar_track_alvar node has 4 main functionalities:\n Generating AR tags of varying size, resolution, and data/ID encoding Identifying and tracking the pose of individual AR tags, optionally integrating kinect depth data (when a kinect is available) for better pose estimates. Identifying and tracking the pose of bundles consisting of multiple tags. This allows for more stable pose estimates, robustness to occlusions, and tracking of multi-sided objects.  Alvar features adaptive thresholding to handle a variety of lighting conditions, optical flow based tracking for more stable pose estimation, and an improved tag identification method that does not significantly slow down as the number of tags increases.\nInstallation for melodic-devel:\nsudo apt-get install ros-melodic-ar-track-alvar ros-melodic-ar-track-alvar-msgs ros-melodic-image-proc  or just clone and build the github repo Github.\nIn order to use AR Marker properly with your camera, be sure to add the camera model to the launch command when using AR Marker (camera_calibration process as described in aprtiltag section.\nGenerating AR tags Two pdf files are in the markers directory containing tags 0-8 and 9-17, respectively. Alternativly, we can get the tags from ar_track_alvar and resize to our use. If you want to generate your own markers with different ID numbers, border widths, or sizes, run:\nrosrun ar_track_alvar createMarker 0 -s 10.0  This will create MarkerData_0.png file that stores a 10cm x 10cm marker with id 0. Print this file on a sheet of paper.\nDue to differences in printer setups, the actual size of the printed marker may be different. Make sure the marker_size parameter represents the actual size (in centimeters) of the AR tag.\nTracking In order to identify and track the poses of (possibly) multiple AR tags that are each considered individually. The packges uses the node individualMarkers which takes the following parameters:\n marker_size (double) \u0026ndash; The width in centimeters of one side of the black square marker border max_new_marker_error (double) \u0026ndash; A threshold determining when new markers can be detected under uncertainty max_track_error (double) \u0026ndash; A threshold determining how much tracking error can be observed before an tag is considered to have disappeared camera_image (string) \u0026ndash; The name of the topic that provides camera frames for detecting the AR tags. This can be mono or color, but should be an UNrectified image, since rectification takes place in this package camera_info (string) \u0026ndash; The name of the topic that provides the camera calibration parameters so that the image can be rectified output_frame (string) \u0026ndash; The name of the frame that the published Cartesian locations of the AR tags will be relative to  IMPORTANT: The node assumes that a Kinect camera being used as the camera, so that depth data can be integrated for better pose estimates. If you are not using a Kinect or do not desire to use depth data improvements, use individualMarkersNoKinect instead.\nTo use the package we need to create a new launch file. Inside launch/ directory add alvar_track.launch with the following content:\n\u0026lt;launch\u0026gt; \u0026lt;arg name=\u0026quot;marker_frame_id\u0026quot; default=\u0026quot;world\u0026quot;/\u0026gt; \u0026lt;arg name=\u0026quot;user_marker_size\u0026quot;\tdefault=\u0026quot;7.0\u0026quot;/\u0026gt; \u0026lt;arg name=\u0026quot;camera_model\u0026quot; default=\u0026quot;astra_pro\u0026quot; doc=\u0026quot;model type [astra_pro, realsense_d435, raspicam]\u0026quot;/\u0026gt; \u0026lt;arg name=\u0026quot;camera_namespace\u0026quot; default=\u0026quot;camera\u0026quot;/\u0026gt; \u0026lt;arg name=\u0026quot;rgb_camera_info_url\u0026quot; default=\u0026quot;package://open_manipulator_p_camera/camera_info/$(arg camera_model).yaml\u0026quot; /\u0026gt; \u0026lt;arg name=\u0026quot;depth_camera_info_url\u0026quot; default=\u0026quot;\u0026quot; /\u0026gt; \u0026lt;include file=\u0026quot;$(find ar_track_alvar)/launch/pr2_indiv_no_kinect.launch\u0026quot;\u0026gt; \u0026lt;arg name=\u0026quot;marker_size\u0026quot; value=\u0026quot;$(arg user_marker_size)\u0026quot; /\u0026gt; \u0026lt;arg name=\u0026quot;max_new_marker_error\u0026quot; value=\u0026quot;0.08\u0026quot; /\u0026gt; \u0026lt;arg name=\u0026quot;max_track_error\u0026quot; value=\u0026quot;0.2\u0026quot; /\u0026gt; \u0026lt;arg name=\u0026quot;cam_image_topic\u0026quot; value=\u0026quot;$(arg camera_namespace)/image_raw\u0026quot; /\u0026gt; \u0026lt;arg name=\u0026quot;cam_info_topic\u0026quot; value=\u0026quot;$(arg camera_namespace)/camera_info\u0026quot; /\u0026gt; \u0026lt;arg name=\u0026quot;output_frame\u0026quot; value=\u0026quot;$(arg marker_frame_id)\u0026quot; /\u0026gt; \u0026lt;/include\u0026gt; \u0026lt;/launch\u0026gt;  Bundle Sometimes it is advantageous to treat \u0026ldquo;bundles\u0026rdquo; of multiple tags as a single unit. For example, this can allow for the estimation of the pose of a many-sided object, even when some of the tags cannot be seen. A tag bundle is defined by an XML file that lists a set of tag IDs and their positions relative to a master tag. The master tag always comes first in the XML file and defines a coordinate system for the rest of the tags.\nIMPORTANT: this coordinate system is different from the standard system used in ROS! In this system, when facing the tag, positive-z comes out of the front of the tag toward the viewer, positive-x is to the right, and positive-y is up.\nTo create a bundle, first choose which tag you want to be the master tag. Treat the center of the master tag as (0,0,0). Then, after placing the rest of the tags, measure the x, y, and z coordinate for each of the 4 corners of all of the tags, relative to the master tag origin. Enter these measurements for each tag into the XML file starting with the lower left corner and progressing counter-clockwise around the tag. After creating the XML file all you need is to add the following parameter to your launch file.\n bundle_files (multiple strings) \u0026ndash; A list of XML file names, one for each bundle you wish to detect.  An example XML file for 2 markers:\n\u0026lt;?xml version=\u0026quot;1.0\u0026quot; encoding=\u0026quot;UTF-8\u0026quot; standalone=\u0026quot;no\u0026quot; ?\u0026gt; \u0026lt;multimarker markers=\u0026quot;2\u0026quot;\u0026gt; \u0026lt;marker index=\u0026quot;8\u0026quot; status=\u0026quot;1\u0026quot;\u0026gt; \u0026lt;corner x=\u0026quot;-2.2\u0026quot; y=\u0026quot;-2.2\u0026quot; z=\u0026quot;0.0\u0026quot; /\u0026gt; \u0026lt;corner x=\u0026quot;2.2\u0026quot; y=\u0026quot;-2.2\u0026quot; z=\u0026quot;0.0\u0026quot; /\u0026gt; \u0026lt;corner x=\u0026quot;2.2\u0026quot; y=\u0026quot;2.2\u0026quot; z=\u0026quot;0.0\u0026quot; /\u0026gt; \u0026lt;corner x=\u0026quot;-2.2\u0026quot; y=\u0026quot;2.2\u0026quot; z=\u0026quot;0.0\u0026quot; /\u0026gt; \u0026lt;/marker\u0026gt; \u0026lt;marker index=\u0026quot;9\u0026quot; status=\u0026quot;1\u0026quot;\u0026gt; \u0026lt;corner x=\u0026quot;-2.2\u0026quot; y=\u0026quot;11.8\u0026quot; z=\u0026quot;0.0\u0026quot; /\u0026gt; \u0026lt;corner x=\u0026quot;2.2\u0026quot; y=\u0026quot;11.8\u0026quot; z=\u0026quot;0.0\u0026quot; /\u0026gt; \u0026lt;corner x=\u0026quot;2.2\u0026quot; y=\u0026quot;16.2\u0026quot; z=\u0026quot;0.0\u0026quot; /\u0026gt; \u0026lt;corner x=\u0026quot;-2.2\u0026quot; y=\u0026quot;16.2\u0026quot; z=\u0026quot;0.0\u0026quot; /\u0026gt; \u0026lt;/marker\u0026gt; \u0026lt;/multimarker\u0026gt;   ArUco Marker Todo.\n OptiTrack Todo.\n ","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1567641600,"objectID":"46bdf36aa53f759a3ba88fb147a54f6f","permalink":"https://osheraz.github.io/blog/track/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/blog/track/","section":"blog","summary":" ","tags":[],"title":"Pose Estimation using Markers","type":"blog"},{"authors":["Osher Azulay"],"categories":[],"content":"\n This post is mainly reproduction of the paper: Simple and Scalable Predictive Uncertainty Estimation using Deep Ensembles (DeepMind, NIPS 2017)   Deep neural networks (NNs) are powerful black box predictors that have recently achieved impressive performance on a wide spectrum of tasks. However, Quantifying their predictive uncertainty is still challenging. Bayesian approximation and ensemble learning techniques are two of the most widely-used uncertainty evaluation methods in the literature. In this outputt ill discuss the implementation of one of the simplest method (which still got great results) to create a deep neural network for estimating predictive uncertainty.\n\nSuch probabilistic models capture the inherent ambiguity in outputs for a given input (aleatoric uncertainty) and ensembles to capture subjective uncertainty (epistemic uncertainty). To that aim, we use a Gaussian parameterization of the form $p_\\theta(y|z)=\\mathcal{N}(\\mu_\\theta(z),\\Sigma_\\theta(z))$ where $y$ is the output and $z$ is the input respectively. The mean $\\mu_\\theta(z)$ and diagonal covariance $\\Sigma_\\theta(z)$ is given by a neural network. With deterministic NN, we can simply output the mean value $\\mu_\\theta(z)$, using mean squared error as the loss function during training $\\mathcal{L}(\\theta)= \\sum_{n}(y-\\mu(z))^{2}$. For probalistic NN, we treat the input $z$ as a sample from Gaussian distribution, while using a negative log-likelihood loss function: \\begin{equation} \\mathcal{L}(\\theta)=-\\log{p_\\theta(y|z)}=\\frac{\\log{\\sigma^{2}_\\theta(zi)}}{2}+\\frac{(y-\\mu_\\theta(z))^{2}}{2}+ const. \\end{equation}\nDuring training we initialized each model $p_\\theta^m$ with different random initialization parameters and different batch of the data and treated each ensemble as a Guassian mixture model. Each ensemble is then treated as Gaussian mixture model: \\begin{equation} \\mathcal{N}(\\mu(z),\\sigma^{2}(z)) = M^{-1}\\sum_{m}{p_{\\theta_m} (y|z)} \\end{equation} where the mean and variance of the mixture is given by: \\begin{equation} \\mu(z) = M^{-1}\\sum_{m}\\mu_{\\theta_m}(z) \\end{equation} \\begin{equation} \\sigma^{2}(z) = M^{-1}\\sum_{m}(\\sigma^{2}_{\\theta_m}(z)+\\mu^{2}_{\\theta_m}(z)) - \\mu(z) \\end{equation}\n\nAlight, lets dig into the implementation, first we import all the necessary packages:\nimport numpy as np import torch import torch.distributions import torch.nn.functional import torch.nn as nn  Then, we will implement a simple Multilayer perceptron:\nclass MLP(nn.Module): def __init__(self, input_dim, output_dim, n_layers, size, device, deterministic, dropout_p, activation = nn.Tanh()): nn.Module.__init__(self) self.deterministic = deterministic self.output_dim = output_dim if not self.deterministic: self.output_dim *= 2 # network architecture self.mlp = nn.ModuleList() self.mlp.append(nn.Linear(input_dim, size)) #first hidden layer self.mlp.append(activation) # self.mlp.append((nn.Dropout(p=dropout_p))) for h in range(n_layers - 1): #additional hidden layers self.mlp.append(nn.Linear(size, size)) self.mlp.append(activation) self.mlp.append((nn.Dropout(p=dropout_p))) self.mlp.append(nn.Linear(size, self.output_dim)) #output layer, no activation function self.to(device) def forward(self, x): for layer in self.mlp: x = layer(x) if self.deterministic: return x else: mean, variance = torch.split(x, int(self.output_dim/2), dim=1) variance = torch.nn.functional.softplus(variance) + 1e-6 return (mean, variance) def save(self, filepath): torch.save(self.state_dict(), filepath, _use_new_zipfile_serialization=False) def restore(self, filepath): self.load_state_dict(torch.load(filepath))  Then we will create the model:\nclass Model: def __init__(self, out_dim, ob_dim, n_layers, size, device, deterministic, optimizer,dropout_p, learning_rate = 0.001): # init vars self.device = device self.deterministic = deterministic self.mlp = MLP(input_dim = ob_dim, output_dim = out_dim, n_layers = n_layers, size = size, device = self.device, deterministic = deterministic, dropout_p= dropout_p) self.optimizer = getattr(torch.optim, optimizer)(self.mlp.parameters(), lr = learning_rate, weight_decay=1e-5) ############################# def get_prediction(self, obs): if len(obs.shape) == 1: obs = np.squeeze(obs)[None] obs = torch.Tensor(obs).to(self.device) if self.deterministic: output = self.mlp(obs).cpu().detach().numpy() return output else: out = self.mlp(obs) output_mean, output_var = out[0].cpu().detach().numpy(), out[1].cpu().detach().numpy() return output_mean, output_var def update(self, observations, true_output): pred_output = self.mlp(torch.Tensor(observations).to(self.device)) true_output = torch.Tensor(true_output).to(self.device) if self.deterministic: loss = nn.functional.mse_loss(true_output, pred_output) else: # Negative log-likelihood loss function. mean = pred_output[0] var = pred_output[1] loss = torch.mean(0.5*torch.log(var) + 0.5*((true_output - mean).pow(2))/var)#.sum() self.optimizer.zero_grad() loss.backward() self.optimizer.step() return loss.item() def eval(self, observations, true_output): with torch.no_grad(): pred_output = self.mlp(torch.Tensor(observations).to(self.device)) true_output = torch.Tensor(true_output).to(self.device) if self.deterministic: loss = nn.functional.mse_loss(true_output, pred_output) else: # Negative log-likelihood loss function. mean = pred_output[0] var = pred_output[1] loss = torch.mean(0.5*torch.log(var) + 0.5*((true_output - mean).pow(2))/var)#.sum() return loss.item() def save_model(self,path): self.mlp.save(path) def load_model(self,path): self.mlp.restore(path)  And finally the Ensemble class:\nclass ModelEnsemble(): def __init__(self, params): # super(ModelEnsemble, self).__init__() self.params = params self.ensemble_size = self.params['ensemble_size'] GEs = [] for i in range(self.ensemble_size): model = Model(self.params['out_dim'], self.params['obs_dim'], self.params['n_layers'], self.params['size'], self.params['device'], self.params['deterministic'], self.params['optimizer'], self.params['dropout_p'], self.params['learning_rate']) GEs.append(model) def forward(self, obs): means = [] variances = [] outputes = [] if GEs[0].deterministic: for model in GEs: output = model.get_prediction(obs) outputes.append(output) mean = np.mean(outputes, axis=0) variance = np.var(outputes, axis=0,ddof=1) else: for model in GEs: mean_m, var_m = model.get_prediction(obs) means.append(mean_m) variances.append(var_m) mean = np.mean(means, axis=0) variance = np.mean((variances + np.power(means, 2)), axis=0) - np.power(mean, 2) return mean, variance def train(self, obs, output): # TODO: each model in the ensemble is trained on a different random batch of size batch_size losses = [] num_data = obs.shape[0] num_data_per_ens = int(num_data / self.ensemble_size) start = 0 for model in GEs: # select which datapoints to use for this model of the ensemble finish = start + num_data_per_ens observations = obs[start:finish] outputes = output[start:finish] # use datapoints to update one of the models loss = model.update(observations, outputes) losses.append(loss) start = finish avg_loss = np.mean(losses) return avg_loss def eval(self, obs, output): losses = [] num_data = obs.shape[0] num_data_per_ens = int(num_data / self.ensemble_size) start = 0 for model in GEs: # select which datapoints to use for this model of the ensemble finish = start + num_data_per_ens observations = obs[start:finish] outputes = output[start:finish] # use datapoints to update one of the models loss = model.eval(observations, outputes) losses.append(loss) start = finish avg_loss = np.mean(losses) return avg_loss def save_models(self,path): for i, model in enumerate(GEs): model.save_model(path + str(i)) def load_models(self,path): GEs = [] for i in range(self.ensemble_size): model = Model(self.params['out_dim'], self.params['obs_dim'], self.params['n_layers'], self.params['size'], self.params['device'], self.params['deterministic'], self.params['optimizer'], self.params['dropout_p'], self.params['learning_rate']) model.load_model(path + str(i)) GEs.append(model)  Toy Example Lets consider the function $y = x^3$ as the ground truth.\nThe dataset is generated by sampling from $y = x^3+\\epsilon$, where $\\epsilon \\sim \\mathcal{N}(0,3^2)$.\nX = torch.tensor([[np.random.uniform(-4,4)] for i in range(20)]) Y = torch.tensor([[x**3 + np.random.normal(0, std=3)] for x in xx]) x = np.linspace(-6, 6, 100).reshape(100, 1) y = x**3 plt.plot(x, y, 'b-', label='ground truth: $y=x^3$') plt.plot(X.numpy(),Y.numpy(),'or', label='data points') plt.grid() plt.xlabel('x') plt.ylabel('y') plt.legend()  computation_graph_args = { 'out_dim': 1, 'ensemble_size': 3, 'n_layers': 3, 'size': 100, 'learning_rate': 0.001, 'device': \u0026quot;cpu\u0026quot;, 'deterministic': False, 'batch_size': 32 * 3, 'display_step': 100, 'epoch': 10, 'optimizer': \u0026quot;Adam\u0026quot;, 'dropout_p': 0.2, }  # Init the Guassian mixture model GE = ModelEnsemble(computation_graph_args)   epochs = computation_graph_args['epoch'] display_step = computation_graph_args['display_step'] COSTS = [] epoch_cost = [] count = 0 for epoch in range(epochs): print(\u0026quot;\\nStart of epoch %d\u0026quot; % (epoch,)) for (batch_idx, batch) in enumerate(train_generator): count += 1 itr = batch_idx batch_x, batch_y = batch['observation'].cpu().detach().numpy(), batch['target'].cpu().detach().numpy() cost = GE.train(batch_x, batch_y) COSTS.append(cost) mean_train_loss = np.mean(COSTS[-len(self.train_generator):]) print('Epoch train loss : ' + str(mean_train_loss)) epoch_cost.append(mean_train_loss) print(\u0026quot;Optimization Finished!\u0026quot;)   means = [] variances = [] for model in GE.models: mean, var = model(torch.tensor(x).float()) mean = mean.detach().numpy() var = var.detach().numpy() means.append(mean) variances.append(var) std = np.sqrt(var) plt.plot(x, mean, label='GMM (NLL) '+str(i+1),alpha=0.5) plt.fill_between(x.reshape(100,), (mean-std).reshape(100,), (mean+std).reshape(100,),alpha=0.1) plt.plot(x, y, label='ground truth $y=x^3$', color='b') plt.plot( X.numpy(),Y.numpy(),'or', label='data points') plt.title('Outputs of the network in the ensemble') plt.xlabel('x') plt.ylabel('y')  ","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1567641600,"objectID":"7c3b48fc04b54a3b02f45e995963bb7c","permalink":"https://osheraz.github.io/blog/ens/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/blog/ens/","section":"blog","summary":" ","tags":[],"title":"Uncertainty propagation in Deep Neural Networks using Ensembles","type":"blog"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"f26b5133c34eec1aa0a09390a36c2ade","permalink":"https://osheraz.github.io/admin/config.yml","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/admin/config.yml","section":"","summary":"","tags":null,"title":"","type":"wowchemycms"}]