<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Well, I should have written that down | Osher Azulay</title><link>https://osheraz.github.io/blog/</link><atom:link href="https://osheraz.github.io/blog/index.xml" rel="self" type="application/rss+xml"/><description>Well, I should have written that down</description><generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Tue, 05 Feb 2019 00:00:00 +0000</lastBuildDate><image><url>https://osheraz.github.io/media/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_2.png</url><title>Well, I should have written that down</title><link>https://osheraz.github.io/blog/</link></image><item><title>Mapping, Perception and More</title><link>https://osheraz.github.io/blog/slam/</link><pubDate>Tue, 05 Feb 2019 00:00:00 +0000</pubDate><guid>https://osheraz.github.io/blog/slam/</guid><description>&lt;p align="center">
&lt;img src="./test.jpg"/>
&lt;/p>
&lt;p>&lt;br />&lt;/p>
&lt;p>Well, I took a really nice course last semester, which was all about autonomous driving. I really liked it because it was a hands-on course, with some of the nicest projects. The course introduced some of the core functions of autonomous driving system, such as localization and mapping, spatial perception and route planning which later on focus on deep learning solutions.&lt;/p>
&lt;p>Throughout the course, I thought it would be great to write a brief description about the projects I have done, for later projects I might do.&lt;/p>
&lt;details class="spoiler " id="spoiler-0">
&lt;summary>Project No.1&lt;/summary>
&lt;p>&lt;p>TODO: add the github folder here&lt;/p>
&lt;h4 id="topics">Topics&lt;/h4>
&lt;ul>
&lt;li>&lt;input checked="" disabled="" type="checkbox"> Mapping and Localization&lt;/li>
&lt;li>&lt;input checked="" disabled="" type="checkbox"> Probabilistic Occupancy Grid&lt;/li>
&lt;li>&lt;input checked="" disabled="" type="checkbox"> Iterative Closet Points (ICP)&lt;/li>
&lt;/ul>
&lt;p>Modeling and understanding the environment is a crucial task for autonomous robotics, in particular for mobile robots. Occupancy Grid Mapping refers to a family of computer algorithms in probabilistic robotics for mobile robots. This address the problem of generating maps from noisy and uncertain sensor measurement data, with the assumption that the robot pose is known. The basic idea of the occupancy grid is to represent a map of the environment as an evenly spaced field of binary random variables each representing the presence of an obstacle at that location in the environment. In this project, we experienced with real captured driving data - &lt;code>KITTI Dataset&lt;/code>. The data-set contains 6 hours of traffic scenarios at 10–100 Hz using a variety of sensor modalities such as high-resolution color and grayscale stereo cameras, a Velodyne 3D laser scanner, and a high-precision GPS/IMU inertial navigation system.&lt;/p>
&lt;p align="center">
&lt;img src="./pykitti.png"/>
&lt;/p>
&lt;p>The goal of an occupancy mapping algorithm is to estimate the posterior probability over maps given the data: $p(m\mid z_{1:t},x_{1:t})$, where $m$ is the map, $z_{1:t}$ is the set of measurements from time 1 to t, and $x_{1:t}$ is the set of robot poses from time 1 to t. If we let $m_{i}$ denote the grid cell with index $i$, then the notation $p(m_{i})$ represents the probability that cell i is occupied. The standard approach, then, is to break the problem down into smaller problems of estimating $p(m_{i}\mid z_{{1:t}},x_{{1:t}})$ for all grid cells $m_{i}$. Each of these estimation problems is then a binary problem. This breakdown is convenient but does lose some of the structure of the problem, since it does not enable modeling dependencies between neighboring cells. Instead, the posterior of a map is approximated by factoring it into $p(m\mid z_{{1:t}},x_{{1:t}})=\prod _{i}p(m_{i}\mid z_{{1:t}},x_{{1:t}})$.&lt;/p>
&lt;p align="center">
&lt;img src="./Video_100x100.gif" width="500" height="500" />
&lt;/p>
&lt;p>Hmm, I think it&amp;rsquo;s enough intro, let&amp;rsquo;s dig into the implementation. For that I used the &lt;code>pykitti&lt;/code> python package, which is a light package of minimal set of tools for working the the KITTI dataset. Basically, after passing the path of the recording, it provides each of the sensors data via generators for easy sequential access. This package assumes that you have already downloaded the calibration data associated with the sequences you want to work on, and that the directory structure is unchanged from the original structure laid out in the KITTI zip files.&lt;/p>
&lt;p>In the GitHub folder above, you can find the implementation of the &lt;code>OccupancyMap&lt;/code> class, which create and update the occupancy &amp;amp; threshold maps. I implemented the update function using the inverse sensor model described in Chapter 9 of “Probabilistic robotics”
by Sebastian Thrun etal [2]. In each of the update functions, I looped over the perceptual
field of the point cloud with respect to the current car position and updated the cell value of the occupancy map using the log odd form of the Bayes filter:
$l(m_{i}|z_{1:t},x_{1:t}) = l(m_{i}|z_{t},x_{t}) + l(m_{i}|z_{1:t−1},x_{1:t−1}) − l_{p}(m_{i})$&lt;/p>
&lt;p>The whole procedure of the occupancy map creation:&lt;/p>
&lt;ol>
&lt;li>At the beginning, I load the dataset and extract the rigid SE(3) pose transformation using &lt;code>dataset.oxts.T_w_imu&lt;/code> w.r.t the first measurement as the origin.&lt;/li>
&lt;li>Load the rigid transformation from the IMU to the velodyne, which is the constant transformation matrix I used to transform the current measured point cloud to the world coordinates at each iteration.&lt;/li>
&lt;li>For each of the car transformation matrix w.r.t to the origin:
&lt;ul>
&lt;li>Get the pose of the car&lt;/li>
&lt;li>Transform the PCL to world coordinate - loop over the current PCL and multiply each point with the current Lidar transformation matrix.&lt;/li>
&lt;li>Importance sampling - get only the cells with more than one measurement in the vertical direction.&lt;/li>
&lt;li>Update the occupancy and threshold maps.&lt;/li>
&lt;li>Visualize the scene map, the current point cloud the current occupancy map.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ol>
&lt;/p>
&lt;/details>
&lt;details class="spoiler " id="spoiler-1">
&lt;summary>Project No.2&lt;/summary>
&lt;p>Todo.&lt;/p>
&lt;/details>
&lt;details class="spoiler " id="spoiler-2">
&lt;summary>Project No.4&lt;/summary>
&lt;p>Todo.&lt;/p>
&lt;/details>
&lt;details class="spoiler " id="spoiler-3">
&lt;summary>Project No.4&lt;/summary>
&lt;p>Todo.&lt;/p>
&lt;/details></description></item><item><title>Pose Estimation using Markers</title><link>https://osheraz.github.io/blog/track/</link><pubDate>Tue, 05 Feb 2019 00:00:00 +0000</pubDate><guid>https://osheraz.github.io/blog/track/</guid><description>&lt;p align="center">
&lt;img src="./body-removebg-preview.png" />
&lt;/p>
&lt;p align = "right">
&lt;sub>&lt;sup>Image courtesy to TagSlam&lt;/sup>&lt;/sub>
&lt;/p>
&lt;br />
&lt;p>Throughout my research, I had to track the $SE(3)$ pose of an object for various of application. In this post ill summarize the &lt;code>ROS&lt;/code>-based implementation of some of the methods. Basically, most of the marker-based methods that ill cover only requires a USB-Cam, except the last one, which requires the whole &lt;code>OptiTrack&lt;/code> motion captures system. We uses &lt;code>OptiTrack&lt;/code> at the lab for for the finer applications.&lt;/p>
&lt;div class="alert alert-note">
&lt;div>
Most of the videos, figures and explanation are taken from the authors implementation and are not mine. I just gather them for future use.
&lt;/div>
&lt;/div>
&lt;p>In the following, ill use the notation $T^{1}_{0}$ to describe the homogeneous transformation matrix between coordinates frame 1 to 0. Detailed explanation about marker-based transformation can be seen &lt;a href="https://berndpfrommer.github.io/tagslam_web/concepts/" target="_blank" rel="noopener">Here&lt;/a>&lt;/p>
&lt;details class="spoiler " id="spoiler-1">
&lt;summary>AprilTag Markers&lt;/summary>
&lt;p>&lt;p>The &lt;a href="https://april.eecs.umich.edu/software/apriltag">AprilTag&lt;/a> come in several different families, depending on how many bits a tag can represent. Some tag families have large, coarse bit blocks (&lt;code>16h5&lt;/code>), while others are fine and smaller (&lt;code>36h11&lt;/code>- most common). For lower resolution camera, consider the &lt;code>16h5&lt;/code> family, For better ill suggest the &lt;code>36h11&lt;/code> and for circular object you should use the &lt;code>21h7&lt;/code> family.&lt;/p>
&lt;p>A full detailed explanation how to generate tags is described:&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://github.com/AprilRobotics/apriltag-generation">AprilTag-Generation&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://berndpfrommer.github.io/tagslam_web/making_tags/">Making and Using AprilTags&lt;/a>&lt;/li>
&lt;/ul>
&lt;p align="center">
&lt;img src="./tag_size_april.jpg"/>
&lt;/p>
&lt;h4 id="measuring-the-tag-size">Measuring the tag Size&lt;/h4>
&lt;p>When entering (ill explain it later) the size of your tag for pose estimation it is important to know what &amp;ldquo;size&amp;rdquo; is actually a measure of. The measurement that is called &amp;ldquo;size&amp;rdquo; is shown as &amp;ldquo;edge length&amp;rdquo; in the image below.
The tag size should not be measured from the outside of the tag. The tag size is defined as the distance between the detection corners, or alternately, the length of the edge between the white border and the black border. The following illustration marks the detection corners with red &lt;code>X&lt;/code>s and the tag size with a red arrow for a tag from the 48h12Custom tag family.&lt;/p>
&lt;p align="center">
&lt;img src="./april_size.png" width=30% />
&lt;/p>
&lt;h4 id="apriltag_roshttpwikirosorgapriltag_ros">&lt;a href="http://wiki.ros.org/apriltag_ros">apriltag_ros&lt;/a>&lt;/h4>
&lt;p align="center">
&lt;img src="./april.png" />
&lt;/p>
&lt;p>The package works as shown in the above figure. The following default input topics are subscribed to (which can be remapped based on your needs):&lt;/p>
&lt;ul>
&lt;li>&lt;code>/camera/image_rect&lt;/code>: a &lt;code>sensor_msgs/Image&lt;/code> topic which contains the image (e.g. a frame of a video stream coming from a camera). The image is assumed to be undistorted, i.e. produced by a pinhole camera. I recommend to use the &lt;a href="http://wiki.ros.org/image_proc">image_proc_node&lt;/a>, which is meant to sit between the camera driver and vision processing nodes. &lt;code>image_proc&lt;/code> removes camera distortion from the raw image stream, and if necessary will convert Bayer or YUV422 format image data to color.&lt;/li>
&lt;li>&lt;code>/camera/camera_info&lt;/code>: a &lt;code>sensor_msgs/CameraInfo&lt;/code> topic which contains the camera calibration matrix in &lt;code>/camera/camera_info/K&lt;/code>. One can obtain a specific camera&amp;rsquo;s K via camera intrinsics calibration using any camera calibarion methods (&lt;a href="http://wiki.ros.org/camera_calibration">Option 1&lt;/a>, &lt;a href="https://navigation.ros.org/tutorials/docs/camera_calibration.html">Option 2&lt;/a>, &lt;a href="https://www.youtube.com/watch?v=z4Oh_9Li72s&amp;amp;ab_channel=RoblabWHGe">Option 3&lt;/a> and so on..).&lt;/li>
&lt;/ul>
&lt;p>The Apriltag ROS package takes in a &lt;code>rectified&lt;/code> camera feed and returns a list of detected tags and their 3D locations. In order for this to work the software needs to know what tags it is looking for and how large they are. These are defined in 2 config files: &lt;code>config/settings.yaml&lt;/code> and &lt;code>config/tags.yaml&lt;/code>.&lt;/p>
&lt;p>The behavior of the ROS wrapper is fully defined by the two configuration files &lt;code>config/tags.yaml&lt;/code> (which defines the tags and tag bundles to look for) and &lt;code>config/settings.yaml&lt;/code> (which configures the core AprilTag 2 algorithm itself). Then, the following topics are output:&lt;/p>
&lt;ul>
&lt;li>&lt;code>/tf&lt;/code>: relative pose between the camera frame and each detected tag&amp;rsquo;s or tag bundle&amp;rsquo;s frame (specified in &lt;code>tags.yaml&lt;/code>) using &lt;code>tf&lt;/code>.&lt;/li>
&lt;li>&lt;code>/tag_detections&lt;/code>: the same information as provided by the &lt;code>/tf&lt;/code> topic but as a custom message carrying the tag ID(s), size(s) and &lt;code>geometry_msgs/PoseWithCovarianceStamped&lt;/code> pose information.&lt;/li>
&lt;li>&lt;code>/tag_detections_image&lt;/code>: the same image as input by &lt;code>/camera/image_rect&lt;/code> but with the detected tags highlighted.&lt;/li>
&lt;/ul>
&lt;h4 id="settingsyaml-parameters">settings.yaml parameters&lt;/h4>
&lt;p>this file configures the detection algorithm parameters (most are self-explained)&lt;/p>
&lt;pre>&lt;code>tag_family: 'tag36h11' # Tag family
tag_border: 1 # Size (in bits) of the black border. Always 1 if made by optitag
tag_threads: 2 # Number of detection thread. Tune per your CPU
tag_decimate: 1.0 # Reduce the resolution of the image by this number. Increases speed at the sacrifice of detecting smaller tags
tag_blur: 0.0 # tag_blur&amp;gt;0 blurs the image and tag_blur&amp;lt;0 sharpens the image
tag_refine_edges: 1 # improves edge detection and therefore improves pose estimation. Light computation
tag_refine_decode: 0 # reduces false negative detection rate. Medium computation
tag_refine_pose: 0 # improves pose estimation accuracy. Heavy computation
tag_debug: 0 # save debug images to ~/.ros Careful if running with video
publish_tf: true # publish tag/bundle poses to /tf topic
&lt;/code>&lt;/pre>
&lt;h4 id="tagsyaml-parameters">tags.yaml parameters&lt;/h4>
&lt;p>This file tells the algorithm what tags to look for in the environment and how large they are so they can be placed in 3D space. The software assumes distance measurements are in &lt;code>meters&lt;/code> and orientation is in &lt;code>quaternions&lt;/code> (One can use &lt;code>tf.transformation&lt;/code> package for other transformations)&lt;/p>
&lt;pre>&lt;code>standalone_tags:
[
{id: 10, size: 0.15},
{id: 20, size: 0.1},
{id: 30, size: 0.07}
]
tag_bundles:
[
{
name: 'my_bundle',
layout:
[
{id: 0, size: 0.05, x: 0.0000, y: 0.0000, z: 0.0, qw: 1.0, qx: 0.0, qy: 0.0, qz: 0.0},
{id: 4, size: 0.05, x: 0.0548, y: -0.0522, z: 0.0, qw: 1.0, qx: 0.0, qy: 0.0, qz: 0.0},
{id: 3, size: 0.05, x: -0.0580, y: -0.0553, z: 0.0, qw: 1.0, qx: 0.0, qy: 0.0, qz: 0.0},
{id: 2, size: 0.05, x: 0.0543, y: 0.0603, z: 0.0, qw: 1.0, qx: 0.0, qy: 0.0, qz: 0.0},
{id: 1, size: 0.05, x: -0.0582, y: 0.0573, z: 0.0, qw: 1.0, qx: 0.0, qy: 0.0, qz: 0.0}
]
}
]
&lt;/code>&lt;/pre>
&lt;p>As we can see in the above configuration, We can define 2 tracking methods, &lt;code>standalone_tags&lt;/code> and &lt;code>tag_bundles&lt;/code>. For &lt;code>standalone&lt;/code> tags, i.e. each marker represents a unique object in the environment we provide an ID and size for each tag you want to detect. For &lt;code>tag_bundles&lt;/code> is the new feature which let you track an object which represented by multiple tags to overcome occlusion and such. Upon detection of a single or multiple tags in the bundle, the software will report the 6 DOF pose of the bundle&amp;rsquo;s origin. When we create a bundle you specify a list of tags. Each tag has a tag ID, size, and 6 DOF location of the tag in reference to the bundle&amp;rsquo;s origin.&lt;/p>
&lt;p>Important notes mentioned by the authors:&lt;/p>
&lt;ul>
&lt;li>No tag ID should appear twice with different sizes (this creates ambiguity in the detection)&lt;/li>
&lt;li>No tag ID should appear twice in the image (this creates ambiguity in the detection)&lt;/li>
&lt;li>It is fine for a tag with the same ID to be listed both in standalone_tags and in tag_bundles, as long as it has the same size.&lt;/li>
&lt;/ul>
&lt;p>A complete &lt;code>launch&lt;/code> file which:&lt;/p>
&lt;ul>
&lt;li>Open the camera stream from the &lt;code>video_device&lt;/code> with &lt;code>width&lt;/code> and &lt;code>height&lt;/code> parameters.&lt;/li>
&lt;li>Launch the &lt;code>image_proc&lt;/code> node to remove distortions.&lt;/li>
&lt;li>Load the &lt;code>settings.yaml&lt;/code> and &lt;code>tags.yaml&lt;/code> configurations files to the parameters server.&lt;/li>
&lt;li>Run the &lt;code>apriltag_ros&lt;/code> core node and publish the detected transformation.&lt;/li>
&lt;/ul>
&lt;/p>
&lt;/details>
&lt;details class="spoiler " id="spoiler-2">
&lt;summary>ArTag Markers&lt;/summary>
&lt;p>Todo.&lt;/p>
&lt;/details>
&lt;details class="spoiler " id="spoiler-3">
&lt;summary>ArUco Marker&lt;/summary>
&lt;p>Todo.&lt;/p>
&lt;/details>
&lt;details class="spoiler " id="spoiler-4">
&lt;summary>OptiTrack&lt;/summary>
&lt;p>Todo.&lt;/p>
&lt;/details></description></item></channel></rss>