<!doctype html><html lang=en-us><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=generator content="Wowchemy 5.2.0 for Hugo"><meta name=author content="Osher Azulay"><meta name=description content=" "><link rel=alternate hreflang=en-us href=https://osheraz.github.io/blog/ens/><link rel=preconnect href=https://fonts.gstatic.com crossorigin><meta name=theme-color content="#1565c0"><script src=/js/mathjax-config.js></script><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/academicons/1.9.0/css/academicons.min.css integrity="sha512-W4yqoT1+8NLkinBLBZko+dFB2ZbHsYLDdr50VElllRcNt2Q4/GSs6u71UHKxB7S6JEMCp5Ve4xjh3eGQl/HRvg==" crossorigin=anonymous><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css integrity="sha256-FMvZuGapsJLjouA6k7Eo2lusoAX9i0ShlWFG6qt7SLc=" crossorigin=anonymous><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/styles/github.min.css crossorigin=anonymous title=hl-light media=print onload="this.media='all'"><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/styles/dracula.min.css crossorigin=anonymous title=hl-dark media=print onload="this.media='all'" disabled><script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js integrity crossorigin=anonymous async></script><link rel=preload as=style href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap"><link rel=stylesheet href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap" media=print onload="this.media='all'"><link rel=stylesheet href=/css/wowchemy.e6f7b650e2d80724e28cce23c674926c.css><link rel=manifest href=/index.webmanifest><link rel=icon type=image/png href=/media/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_32x32_fill_lanczos_center_2.png><link rel=apple-touch-icon type=image/png href=/media/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_180x180_fill_lanczos_center_2.png><link rel=canonical href=https://osheraz.github.io/blog/ens/><meta property="twitter:card" content="summary"><meta property="og:site_name" content="Osher Azulay"><meta property="og:url" content="https://osheraz.github.io/blog/ens/"><meta property="og:title" content="Uncertainty propagation in Deep Neural Networks using Ensembles | Osher Azulay"><meta property="og:description" content=" "><meta property="og:image" content="https://osheraz.github.io/media/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_2.png"><meta property="twitter:image" content="https://osheraz.github.io/media/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_2.png"><meta property="og:locale" content="en-us"><meta property="article:published_time" content="2019-02-05T00:00:00+00:00"><meta property="article:modified_time" content="2019-09-05T00:00:00+00:00"><title>Uncertainty propagation in Deep Neural Networks using Ensembles | Osher Azulay</title></head><body id=top data-spy=scroll data-offset=70 data-target=#TableOfContents class=page-wrapper data-wc-page-id=7c3b48fc04b54a3b02f45e995963bb7c><script src=/js/wowchemy-init.min.b8153d4570dcbb34350a2a846dba8c03.js></script><aside class=search-modal id=search><div class=container><section class=search-header><div class="row no-gutters justify-content-between mb-3"><div class=col-6><h1>Search</h1></div><div class="col-6 col-search-close"><a class=js-search href=# aria-label=Close><i class="fas fa-times-circle text-muted" aria-hidden=true></i></a></div></div><div id=search-box><input name=q id=search-query placeholder=Search... autocapitalize=off autocomplete=off autocorrect=off spellcheck=false type=search class=form-control aria-label=Search...></div></section><section class=section-search-results><div id=search-hits></div></section></div></aside><div class=page-header><nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id=navbar-main><div class=container-xl><div class="d-none d-lg-inline-flex"><a class=navbar-brand href=/>Osher Azulay</a></div><button type=button class=navbar-toggler data-toggle=collapse data-target=#navbar-content aria-controls=navbar-content aria-expanded=false aria-label="Toggle navigation">
<span><i class="fas fa-bars"></i></span></button><div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none"><a class=navbar-brand href=/>Osher Azulay</a></div><div class="navbar-collapse main-menu-item collapse justify-content-start" id=navbar-content><ul class="navbar-nav d-md-inline-flex"><li class=nav-item><a class=nav-link href=/#about><span>Home</span></a></li><li class=nav-item><a class=nav-link href=/#featured><span>Publications</span></a></li><li class=nav-item><a class="nav-link active" href=/blog/><span>Under Construction</span></a></li></ul></div><ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2"><li class=nav-item><a class="nav-link js-search" href=# aria-label=Search><i class="fas fa-search" aria-hidden=true></i></a></li><li class="nav-item dropdown theme-dropdown"><a href=# class=nav-link data-toggle=dropdown aria-haspopup=true aria-label="Display preferences"><i class="fas fa-moon" aria-hidden=true></i></a><div class=dropdown-menu><a href=# class="dropdown-item js-set-theme-light"><span>Light</span></a>
<a href=# class="dropdown-item js-set-theme-dark"><span>Dark</span></a>
<a href=# class="dropdown-item js-set-theme-auto"><span>Automatic</span></a></div></li></ul></div></nav></div><div class=page-body><article class=article><div class="article-container pt-3"><h1>Uncertainty propagation in Deep Neural Networks using Ensembles</h1><p class=page-subtitle></p><div class=article-metadata><div><span>Osher Azulay</span></div><span class=article-date>Last updated on
Sep 5, 2019</span></div></div><div class=article-container><div class=article-style><p align=center><img src=./prop.png width=60%></p><p><br></p><div class="alert alert-note"><div>This post is mainly reproduction of the paper: <code>Simple and Scalable Predictive Uncertainty Estimation using Deep Ensembles (DeepMind, NIPS 2017)</code></div></div><p>Deep neural networks (NNs) are powerful black box predictors that have recently
achieved impressive performance on a wide spectrum of tasks. However, Quantifying their predictive uncertainty is still challenging. <code>Bayesian approximation</code> and <code>ensemble learning</code> techniques are two of the most widely-used uncertainty evaluation methods in the literature. In this outputt ill discuss the implementation of one of the simplest method (which still got great results) to create a deep neural network for estimating predictive uncertainty.</p><p><br></p><p>Such probabilistic models capture the inherent ambiguity in outputs for a given input (aleatoric uncertainty) and ensembles to capture subjective uncertainty (epistemic uncertainty). To that aim, we use a Gaussian parameterization of the form $p_\theta(y|z)=\mathcal{N}(\mu_\theta(z),\Sigma_\theta(z))$ where $y$ is the output and $z$ is the input respectively. The mean $\mu_\theta(z)$ and diagonal covariance $\Sigma_\theta(z)$ is given by a neural network. With deterministic NN, we can simply output the mean value $\mu_\theta(z)$, using <code>mean squared error</code> as the loss function during training $\mathcal{L}(\theta)= \sum_{n}(y-\mu(z))^{2}$. For probalistic NN, we treat the input $z$ as a sample from Gaussian distribution, while using a <code>negative log-likelihood loss</code> function:
\begin{equation}
\mathcal{L}(\theta)=-\log{p_\theta(y|z)}=\frac{\log{\sigma^{2}_\theta(zi)}}{2}+\frac{(y-\mu_\theta(z))^{2}}{2}+ const.
\end{equation}</p><p align=center><img src=./ens1.png width=100%></p><p>During training we initialized each model $p_\theta^m$ with different random initialization parameters and different batch of the data and treated each ensemble as a Guassian mixture model. Each ensemble is then treated as Gaussian mixture model:
\begin{equation}
\mathcal{N}(\mu(z),\sigma^{2}(z)) = M^{-1}\sum_{m}{p_{\theta_m} (y|z)}
\end{equation}
where the mean and variance of the mixture is given by:
\begin{equation}
\mu(z) = M^{-1}\sum_{m}\mu_{\theta_m}(z)
\end{equation}
\begin{equation}
\sigma^{2}(z) = M^{-1}\sum_{m}(\sigma^{2}_{\theta_m}(z)+\mu^{2}_{\theta_m}(z)) - \mu(z)
\end{equation}</p><p><br></p><p>Alight, lets dig into the implementation, first we import all the necessary packages:</p><pre><code>import numpy as np
import torch
import torch.distributions
import torch.nn.functional
import torch.nn as nn
</code></pre><p>Then, we will implement a simple <code>Multilayer perceptron</code>:</p><pre><code>class MLP(nn.Module):
    def __init__(self,
        input_dim,
        output_dim,
        n_layers,
        size,
        device,
        deterministic,
        dropout_p,
        activation = nn.Tanh()):
        nn.Module.__init__(self)

        self.deterministic = deterministic
        self.output_dim = output_dim

        if not self.deterministic:
            self.output_dim *= 2

        # network architecture
        self.mlp = nn.ModuleList()
        self.mlp.append(nn.Linear(input_dim, size)) #first hidden layer
        self.mlp.append(activation)
        # self.mlp.append((nn.Dropout(p=dropout_p)))

        for h in range(n_layers - 1): #additional hidden layers
            self.mlp.append(nn.Linear(size, size))
            self.mlp.append(activation)
            self.mlp.append((nn.Dropout(p=dropout_p)))

        self.mlp.append(nn.Linear(size, self.output_dim)) #output layer, no activation function

        self.to(device)

    def forward(self, x):
        for layer in self.mlp:
            x = layer(x)
        if self.deterministic:
            return x
        else:
            mean, variance = torch.split(x, int(self.output_dim/2), dim=1)
            variance = torch.nn.functional.softplus(variance) + 1e-6
            return (mean, variance)

    def save(self, filepath):
        torch.save(self.state_dict(), filepath, _use_new_zipfile_serialization=False)

    def restore(self, filepath):
        self.load_state_dict(torch.load(filepath))

</code></pre><p>Then we will create the model:</p><pre><code>class Model:
    def __init__(self, out_dim, ob_dim, n_layers, size, device, deterministic, optimizer,dropout_p, learning_rate = 0.001):
        # init vars
        self.device = device
        self.deterministic = deterministic

        self.mlp = MLP(input_dim = ob_dim,
                              output_dim = out_dim,
                              n_layers = n_layers,
                              size = size,
                              device = self.device,
                              deterministic = deterministic,
                              dropout_p= dropout_p)


        self.optimizer = getattr(torch.optim, optimizer)(self.mlp.parameters(), lr = learning_rate, weight_decay=1e-5)

    #############################

    def get_prediction(self, obs):

        if len(obs.shape) == 1:
            obs = np.squeeze(obs)[None]

        obs = torch.Tensor(obs).to(self.device)

        if self.deterministic:
            output = self.mlp(obs).cpu().detach().numpy()
            return output
        else:
            out = self.mlp(obs)
            output_mean, output_var = out[0].cpu().detach().numpy(), out[1].cpu().detach().numpy()
            return output_mean, output_var

    def update(self, observations, true_output):

        pred_output = self.mlp(torch.Tensor(observations).to(self.device))
        true_output = torch.Tensor(true_output).to(self.device)

        if self.deterministic:
            loss = nn.functional.mse_loss(true_output, pred_output)
        else:
            # Negative log-likelihood loss function.
            mean = pred_output[0]
            var = pred_output[1]
            loss = torch.mean(0.5*torch.log(var) + 0.5*((true_output - mean).pow(2))/var)#.sum()

        self.optimizer.zero_grad()
        loss.backward()
        self.optimizer.step()

        return loss.item()

    def eval(self, observations, true_output):

        with torch.no_grad():
            pred_output = self.mlp(torch.Tensor(observations).to(self.device))

        true_output = torch.Tensor(true_output).to(self.device)

        if self.deterministic:
            loss = nn.functional.mse_loss(true_output, pred_output)
        else:
            # Negative log-likelihood loss function.
            mean = pred_output[0]
            var = pred_output[1]
            loss = torch.mean(0.5*torch.log(var) + 0.5*((true_output - mean).pow(2))/var)#.sum()

        return loss.item()

    def save_model(self,path):
        self.mlp.save(path)

    def load_model(self,path):
        self.mlp.restore(path)
</code></pre><p>And finally the Ensemble class:</p><pre><code>class ModelEnsemble():
    def __init__(self, params):
        # super(ModelEnsemble, self).__init__()

        self.params = params
        self.ensemble_size = self.params['ensemble_size']

        GEs = []
        for i in range(self.ensemble_size):
            model = Model(self.params['out_dim'],
                            self.params['obs_dim'],
                            self.params['n_layers'],
                            self.params['size'],
                            self.params['device'],
                            self.params['deterministic'],
                            self.params['optimizer'],
                            self.params['dropout_p'],
                            self.params['learning_rate'])

            GEs.append(model)

    def forward(self,  obs):
        means = []
        variances = []
        outputes = []

        if GEs[0].deterministic:
            for model in GEs:
                output = model.get_prediction(obs)
                outputes.append(output)
            mean = np.mean(outputes, axis=0)
            variance = np.var(outputes, axis=0,ddof=1)
        else:
            for model in GEs:
                mean_m, var_m = model.get_prediction(obs)
                means.append(mean_m)
                variances.append(var_m)
            mean = np.mean(means, axis=0)
            variance = np.mean((variances + np.power(means, 2)), axis=0) - np.power(mean, 2)

        return mean, variance

    def train(self, obs, output):

        # TODO: each model in the ensemble is trained on a different random batch of size batch_size
        losses = []
        num_data = obs.shape[0]
        num_data_per_ens = int(num_data / self.ensemble_size)

        start = 0
        for model in GEs:
            # select which datapoints to use for this model of the ensemble
            finish = start + num_data_per_ens

            observations = obs[start:finish]
            outputes = output[start:finish]

            # use datapoints to update one of the models
            loss = model.update(observations, outputes)
            losses.append(loss)

            start = finish

        avg_loss = np.mean(losses)

        return avg_loss

    def eval(self, obs, output):

        losses = []
        num_data = obs.shape[0]
        num_data_per_ens = int(num_data / self.ensemble_size)

        start = 0
        for model in GEs:
            # select which datapoints to use for this model of the ensemble
            finish = start + num_data_per_ens

            observations = obs[start:finish]
            outputes = output[start:finish]

            # use datapoints to update one of the models
            loss = model.eval(observations, outputes)
            losses.append(loss)

            start = finish

        avg_loss = np.mean(losses)

        return avg_loss

    def save_models(self,path):
        for i, model in enumerate(GEs):
            model.save_model(path + str(i))

    def load_models(self,path):

        GEs = []
        for i in range(self.ensemble_size):
            model = Model(self.params['out_dim'],
                            self.params['obs_dim'],
                            self.params['n_layers'],
                            self.params['size'],
                            self.params['device'],
                            self.params['deterministic'],
                            self.params['optimizer'],
                            self.params['dropout_p'],
                            self.params['learning_rate'])
            model.load_model(path + str(i))
            GEs.append(model)
</code></pre><h2 id=toy-example>Toy Example</h2><p>Lets consider the function $y = x^3$ as the ground truth.</p><p>The dataset is generated by sampling from $y = x^3+\epsilon$, where $\epsilon \sim \mathcal{N}(0,3^2)$.</p><p align=center><img src=./p1.png width=50%></p><pre><code>X = torch.tensor([[np.random.uniform(-4,4)] for i in range(20)])
Y = torch.tensor([[x**3 + np.random.normal(0, std=3)] for x in xx])
x = np.linspace(-6, 6, 100).reshape(100, 1)
y = x**3
plt.plot(x, y, 'b-', label='ground truth: $y=x^3$')
plt.plot(X.numpy(),Y.numpy(),'or', label='data points')
plt.grid()
plt.xlabel('x')
plt.ylabel('y')
plt.legend()
</code></pre><pre><code>computation_graph_args = {
    'out_dim': 1,
    'ensemble_size': 3,
    'n_layers': 3,
    'size': 100,
    'learning_rate': 0.001,
    'device': &quot;cpu&quot;,
    'deterministic': False,
    'batch_size': 32 * 3,
    'display_step': 100,
    'epoch': 10,
    'optimizer': &quot;Adam&quot;,
    'dropout_p': 0.2,
}
</code></pre><pre><code># Init the Guassian mixture model
GE = ModelEnsemble(computation_graph_args)
</code></pre><pre><code>

    epochs = computation_graph_args['epoch']
    display_step = computation_graph_args['display_step']

    COSTS = []
    epoch_cost = []
    count = 0

    for epoch in range(epochs):
        print(&quot;\nStart of epoch %d&quot; % (epoch,))

        for (batch_idx, batch) in enumerate(train_generator):
            count += 1
            itr = batch_idx

            batch_x, batch_y = batch['observation'].cpu().detach().numpy(), batch['target'].cpu().detach().numpy()

            cost = GE.train(batch_x, batch_y)
            COSTS.append(cost)

        mean_train_loss = np.mean(COSTS[-len(self.train_generator):])
        print('Epoch train loss : ' + str(mean_train_loss))

        epoch_cost.append(mean_train_loss)

    print(&quot;Optimization Finished!&quot;)
</code></pre><pre><code>
means = []
variances = []
for model in GE.models:
    mean, var = model(torch.tensor(x).float())
    mean = mean.detach().numpy()
    var = var.detach().numpy()
    means.append(mean)
    variances.append(var)
    std = np.sqrt(var)
    plt.plot(x, mean, label='GMM (NLL) '+str(i+1),alpha=0.5)
    plt.fill_between(x.reshape(100,), (mean-std).reshape(100,), (mean+std).reshape(100,),alpha=0.1)
plt.plot(x, y, label='ground truth $y=x^3$', color='b')
  plt.plot( X.numpy(),Y.numpy(),'or', label='data points')
plt.title('Outputs of the network in the ensemble')
plt.xlabel('x')
plt.ylabel('y')
</code></pre><p align=center><img src=./p2.png width=40%>
<img src=./p3.png width=40%></p></div><div class=share-box aria-hidden=true><ul class=share><li><a href="https://twitter.com/intent/tweet?url=https://osheraz.github.io/blog/ens/&text=Uncertainty%20propagation%20in%20Deep%20Neural%20Networks%20using%20Ensembles" target=_blank rel=noopener class=share-btn-twitter><i class="fab fa-twitter"></i></a></li><li><a href="https://www.facebook.com/sharer.php?u=https://osheraz.github.io/blog/ens/&t=Uncertainty%20propagation%20in%20Deep%20Neural%20Networks%20using%20Ensembles" target=_blank rel=noopener class=share-btn-facebook><i class="fab fa-facebook"></i></a></li><li><a href="mailto:?subject=Uncertainty%20propagation%20in%20Deep%20Neural%20Networks%20using%20Ensembles&body=https://osheraz.github.io/blog/ens/" target=_blank rel=noopener class=share-btn-email><i class="fas fa-envelope"></i></a></li><li><a href="https://www.linkedin.com/shareArticle?url=https://osheraz.github.io/blog/ens/&title=Uncertainty%20propagation%20in%20Deep%20Neural%20Networks%20using%20Ensembles" target=_blank rel=noopener class=share-btn-linkedin><i class="fab fa-linkedin-in"></i></a></li><li><a href="whatsapp://send?text=Uncertainty%20propagation%20in%20Deep%20Neural%20Networks%20using%20Ensembles%20https://osheraz.github.io/blog/ens/" target=_blank rel=noopener class=share-btn-whatsapp><i class="fab fa-whatsapp"></i></a></li><li><a href="https://service.weibo.com/share/share.php?url=https://osheraz.github.io/blog/ens/&title=Uncertainty%20propagation%20in%20Deep%20Neural%20Networks%20using%20Ensembles" target=_blank rel=noopener class=share-btn-weibo><i class="fab fa-weibo"></i></a></li></ul></div><div class="media author-card content-widget-hr"><a href=https://osheraz.github.io/><img class="avatar mr-3 avatar-circle" src=/authors/osher-azulay/avatar_hub134170bfb8ac65f2ae4f5f140cab5f2_67039_270x270_fill_q75_lanczos_center.jpg alt="Osher Azulay"></a><div class=media-body><h5 class=card-title><a href=https://osheraz.github.io/>Osher Azulay</a></h5><h6 class=card-subtitle>Roboticist</h6><p class=card-text>My research interests include robotic manipulation, deep reinforcement learning and tactile sensing.</p><ul class=network-icon aria-hidden=true><li><a href=/#contact><i class="fas fa-envelope"></i></a></li><li><a href=https://github.com/osheraz target=_blank rel=noopener><i class="fab fa-github"></i></a></li><li><a href=https://www.linkedin.com/in/osher-azulay-20ab38154/ target=_blank rel=noopener><i class="fab fa-linkedin"></i></a></li><li><a href=/uploads/Osher%20CV.pdf><i class="ai ai-cv"></i></a></li></ul></div></div></div></article></div><div class=page-footer><div class=container><footer class=site-footer><p class=powered-by>Published with <a href="https://wowchemy.com/?utm_campaign=poweredby" target=_blank rel=noopener>Wowchemy</a> — the free, <a href=https://github.com/wowchemy/wowchemy-hugo-modules target=_blank rel=noopener>open source</a> website builder that empowers creators.</p></footer></div></div><div id=modal class="modal fade" role=dialog><div class=modal-dialog><div class=modal-content><div class=modal-header><h5 class=modal-title>Cite</h5><button type=button class=close data-dismiss=modal aria-label=Close>
<span aria-hidden=true>&#215;</span></button></div><div class=modal-body><pre><code class="tex hljs"></code></pre></div><div class=modal-footer><a class="btn btn-outline-primary my-1 js-copy-cite" href=# target=_blank><i class="fas fa-copy"></i> Copy</a>
<a class="btn btn-outline-primary my-1 js-download-cite" href=# target=_blank><i class="fas fa-download"></i> Download</a><div id=modal-error></div></div></div></div></div><script src=https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js integrity="sha256-9/aliU8dGd2tb6OSsuzixeV4y/faTqgFtohetphbbj0=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/instant.page/5.1.0/instantpage.min.js integrity="sha512-1+qUtKoh9XZW7j+6LhRMAyOrgSQKenQ4mluTR+cvxXjP1Z54RxZuzstR/H9kgPXQsVB8IW7DMDFUJpzLjvhGSQ==" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.4/imagesloaded.pkgd.min.js integrity="sha256-lqvxZrPLtfffUl2G/e7szqSvPBILGbwmsGE1MKlOi0Q=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.6/isotope.pkgd.min.js integrity="sha256-CBrpuqrMhXwcLLUd5tvQ4euBHCdh7wGlDfNz8vbu/iI=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/highlight.min.js integrity="sha512-TDKKr+IvoqZnPzc3l35hdjpHD0m+b2EC2SrLEgKDRWpxf2rFCxemkgvJ5kfU48ip+Y+m2XVKyOCD85ybtlZDmw==" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/languages/r.min.js></script><script id=search-hit-fuse-template type=text/x-template>
        <div class="search-hit" id="summary-{{key}}">
          <div class="search-hit-content">
            <div class="search-hit-name">
              <a href="{{relpermalink}}">{{title}}</a>
              <div class="article-metadata search-hit-type">{{type}}</div>
              <p class="search-hit-description">{{snippet}}</p>
            </div>
          </div>
        </div>
      </script><script src=https://cdnjs.cloudflare.com/ajax/libs/fuse.js/3.2.1/fuse.min.js integrity="sha256-VzgmKYmhsGNNN4Ph1kMW+BjoYJM2jV5i4IlFoeZA9XI=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/jquery.mark.min.js integrity="sha256-4HLtjeVgH0eIB3aZ9mLYF6E8oU5chNdjU6p6rrXpl9U=" crossorigin=anonymous></script><script src=/js/bootstrap.bundle.min.6aed84840afc03ab4d5750157f69c120.js></script><script src=/en/js/wowchemy.min.8626d678bd286b6b2b19961df891e128.js></script></body></html>