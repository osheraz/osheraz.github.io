<!doctype html><html lang=en-us><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=generator content="Wowchemy 5.2.0 for Hugo"><meta name=author content="Osher Azulay"><meta name=description content=" "><link rel=alternate hreflang=en-us href=https://osheraz.github.io/blog/track/><link rel=preconnect href=https://fonts.gstatic.com crossorigin><meta name=theme-color content="#1565c0"><script src=/js/mathjax-config.js></script><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/academicons/1.9.0/css/academicons.min.css integrity="sha512-W4yqoT1+8NLkinBLBZko+dFB2ZbHsYLDdr50VElllRcNt2Q4/GSs6u71UHKxB7S6JEMCp5Ve4xjh3eGQl/HRvg==" crossorigin=anonymous><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css integrity="sha256-FMvZuGapsJLjouA6k7Eo2lusoAX9i0ShlWFG6qt7SLc=" crossorigin=anonymous><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/styles/github.min.css crossorigin=anonymous title=hl-light media=print onload="this.media='all'"><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/styles/dracula.min.css crossorigin=anonymous title=hl-dark media=print onload="this.media='all'" disabled><script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js integrity crossorigin=anonymous async></script><link rel=preload as=style href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap"><link rel=stylesheet href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap" media=print onload="this.media='all'"><link rel=stylesheet href=/css/wowchemy.e6f7b650e2d80724e28cce23c674926c.css><link rel=manifest href=/index.webmanifest><link rel=icon type=image/png href=/media/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_32x32_fill_lanczos_center_2.png><link rel=apple-touch-icon type=image/png href=/media/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_180x180_fill_lanczos_center_2.png><link rel=canonical href=https://osheraz.github.io/blog/track/><meta property="twitter:card" content="summary"><meta property="og:site_name" content="Osher Azulay"><meta property="og:url" content="https://osheraz.github.io/blog/track/"><meta property="og:title" content="Pose Estimation using Markers | Osher Azulay"><meta property="og:description" content=" "><meta property="og:image" content="https://osheraz.github.io/media/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_2.png"><meta property="twitter:image" content="https://osheraz.github.io/media/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_2.png"><meta property="og:locale" content="en-us"><meta property="article:published_time" content="2019-02-05T00:00:00+00:00"><meta property="article:modified_time" content="2019-09-05T00:00:00+00:00"><title>Pose Estimation using Markers | Osher Azulay</title></head><body id=top data-spy=scroll data-offset=70 data-target=#TableOfContents class=page-wrapper data-wc-page-id=46bdf36aa53f759a3ba88fb147a54f6f><script src=/js/wowchemy-init.min.b8153d4570dcbb34350a2a846dba8c03.js></script><aside class=search-modal id=search><div class=container><section class=search-header><div class="row no-gutters justify-content-between mb-3"><div class=col-6><h1>Search</h1></div><div class="col-6 col-search-close"><a class=js-search href=# aria-label=Close><i class="fas fa-times-circle text-muted" aria-hidden=true></i></a></div></div><div id=search-box><input name=q id=search-query placeholder=Search... autocapitalize=off autocomplete=off autocorrect=off spellcheck=false type=search class=form-control aria-label=Search...></div></section><section class=section-search-results><div id=search-hits></div></section></div></aside><div class=page-header><nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id=navbar-main><div class=container-xl><div class="d-none d-lg-inline-flex"><a class=navbar-brand href=/>Osher Azulay</a></div><button type=button class=navbar-toggler data-toggle=collapse data-target=#navbar-content aria-controls=navbar-content aria-expanded=false aria-label="Toggle navigation">
<span><i class="fas fa-bars"></i></span></button><div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none"><a class=navbar-brand href=/>Osher Azulay</a></div><div class="navbar-collapse main-menu-item collapse justify-content-start" id=navbar-content><ul class="navbar-nav d-md-inline-flex"><li class=nav-item><a class=nav-link href=/#about><span>Home</span></a></li><li class=nav-item><a class=nav-link href=/#featured><span>Publications</span></a></li><li class=nav-item><a class="nav-link active" href=/blog/><span>Blog</span></a></li></ul></div><ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2"><li class=nav-item><a class="nav-link js-search" href=# aria-label=Search><i class="fas fa-search" aria-hidden=true></i></a></li><li class="nav-item dropdown theme-dropdown"><a href=# class=nav-link data-toggle=dropdown aria-haspopup=true aria-label="Display preferences"><i class="fas fa-moon" aria-hidden=true></i></a><div class=dropdown-menu><a href=# class="dropdown-item js-set-theme-light"><span>Light</span></a>
<a href=# class="dropdown-item js-set-theme-dark"><span>Dark</span></a>
<a href=# class="dropdown-item js-set-theme-auto"><span>Automatic</span></a></div></li></ul></div></nav></div><div class=page-body><article class=article><div class="article-container pt-3"><h1>Pose Estimation using Markers</h1><p class=page-subtitle></p><div class=article-metadata><div><span>Osher Azulay</span></div><span class=article-date>Last updated on
Sep 5, 2019</span></div></div><div class=article-container><div class=article-style><p align=center><img src=./body-removebg-preview.png></p><p align=right><sub><sup>Image courtesy to TagSlam</sup></sub></p><br><p>Throughout my research, I had to track the $SE(3)$ pose of an object for various of application. In this post ill summarize the <code>ROS</code>-based implementation of some of the methods. Basically, most of the marker-based methods that ill cover only requires a USB-Cam, except the last one, which requires the whole <code>OptiTrack</code> motion captures system. We uses <code>OptiTrack</code> at the lab for for the finer applications.</p><div class="alert alert-note"><div>Most of the videos, figures and explanation are taken from the authors implementation and are not mine. I just gather them for future use.</div></div><p>In the following, ill use the notation $T^{1}_{0}$ to describe the homogeneous transformation matrix between coordinates frame 1 to 0. Detailed explanation about marker-based transformation can be seen <a href=https://berndpfrommer.github.io/tagslam_web/concepts/ target=_blank rel=noopener>Here</a></p><details class=spoiler id=spoiler-1><summary>AprilTag Markers</summary><p><p>The <a href=https://april.eecs.umich.edu/software/apriltag>AprilTag</a> come in several different families, depending on how many bits a tag can represent. Some tag families have large, coarse bit blocks (<code>16h5</code>), while others are fine and smaller (<code>36h11</code>- most common). For lower resolution camera, consider the <code>16h5</code> family, For better ill suggest the <code>36h11</code> and for circular object you should use the <code>21h7</code> family.</p><p>A full detailed explanation how to generate tags is described:</p><ul><li><a href=https://github.com/AprilRobotics/apriltag-generation>AprilTag-Generation</a></li><li><a href=https://berndpfrommer.github.io/tagslam_web/making_tags/>Making and Using AprilTags</a></li></ul><p align=center><img src=./tag_size_april.jpg></p><h4 id=measuring-the-tag-size>Measuring the tag Size</h4><p>When entering (ill explain it later) the size of your tag for pose estimation it is important to know what &ldquo;size&rdquo; is actually a measure of. The measurement that is called &ldquo;size&rdquo; is shown as &ldquo;edge length&rdquo; in the image below.
The tag size should not be measured from the outside of the tag. The tag size is defined as the distance between the detection corners, or alternately, the length of the edge between the white border and the black border. The following illustration marks the detection corners with red <code>X</code>s and the tag size with a red arrow for a tag from the 48h12Custom tag family.</p><p align=center><img src=./april_size.png width=30%></p><h4 id=coordinate-system>Coordinate system</h4><p>The coordinate system has the origin at the camera center. The z-axis points from the camera center out the camera lens. The x-axis is to the right in the image taken by the camera, and y is up.</p><p align=center><img src=./test.png width=30%></p><h3 id=apriltag_roshttpwikirosorgapriltag_ros><a href=http://wiki.ros.org/apriltag_ros>apriltag_ros</a></h3><h4 id=installation>Installation</h4><p>Starting with a working ROS installation (Kinetic and Melodic are supported):</p><pre><code>export ROS_DISTRO=melodic               # Set this to your distro, e.g. kinetic or melodic
source /opt/ros/$ROS_DISTRO/setup.bash  # Source your ROS distro
mkdir -p ~/catkin_ws/src                # Make a new workspace
cd ~/catkin_ws/src                      # Navigate to the source space
git clone https://github.com/AprilRobotics/apriltag.git      # Clone Apriltag library
git clone https://github.com/AprilRobotics/apriltag_ros.git  # Clone Apriltag ROS wrapper
cd ~/catkin_ws                          # Navigate to the workspace
rosdep install --from-paths src --ignore-src -r -y  # Install any missing packages
catkin build    # Build all packages in the workspace (catkin_make_isolated will work also)
</code></pre><p>The package works as shown in the below figure. The following default input topics are subscribed to (which can be remapped based on your needs):</p><p align=center><img src=./april.png></p><ul><li><code>/camera/image_rect</code>: a <code>sensor_msgs/Image</code> topic which contains the image (e.g. a frame of a video stream coming from a camera). The image is assumed to be undistorted, i.e. produced by a pinhole camera. I recommend to use the <a href=http://wiki.ros.org/image_proc>image_proc_node</a>, which is meant to sit between the camera driver and vision processing nodes. <code>image_proc</code> removes camera distortion from the raw image stream, and if necessary will convert Bayer or YUV422 format image data to color.</li><li><code>/camera/camera_info</code>: a <code>sensor_msgs/CameraInfo</code> topic which contains the camera calibration matrix in <code>/camera/camera_info/K</code>. One can obtain a specific camera&rsquo;s K via camera intrinsics calibration using any camera calibarion methods (<a href=http://wiki.ros.org/camera_calibration>Option 1</a>, <a href=https://navigation.ros.org/tutorials/docs/camera_calibration.html>Option 2</a>, <a href="https://www.youtube.com/watch?v=z4Oh_9Li72s&ab_channel=RoblabWHGe">Option 3</a> and so on..).</li></ul><p>The Apriltag ROS package takes in a <code>rectified</code> camera feed and returns a list of detected tags and their 3D locations. In order for this to work the software needs to know what tags it is looking for and how large they are. These are defined in 2 config files: <code>config/settings.yaml</code> and <code>config/tags.yaml</code>.</p><p>The behavior of the ROS wrapper is fully defined by the two configuration files <code>config/tags.yaml</code> (which defines the tags and tag bundles to look for) and <code>config/settings.yaml</code> (which configures the core AprilTag 2 algorithm itself). Then, the following topics are output:</p><ul><li><code>/tf</code>: relative pose between the camera frame and each detected tag&rsquo;s or tag bundle&rsquo;s frame (specified in <code>tags.yaml</code>) using <code>tf</code>.</li><li><code>/tag_detections</code>: the same information as provided by the <code>/tf</code> topic but as a custom message carrying the tag ID(s), size(s) and <code>geometry_msgs/PoseWithCovarianceStamped</code> pose information.</li><li><code>/tag_detections_image</code>: the same image as input by <code>/camera/image_rect</code> but with the detected tags highlighted.</li></ul><h4 id=settingsyaml-parameters>settings.yaml parameters</h4><p>this file configures the detection algorithm parameters (most are self-explained)</p><pre><code>tag_family:        'tag36h11' # Tag family
tag_border:        1          # Size (in bits) of the black border. Always 1 if made by optitag
tag_threads:       2          # Number of detection thread. Tune per your CPU
tag_decimate:      1.0        # Reduce the resolution of the image by this number. Increases speed at the sacrifice of detecting smaller tags
tag_blur:          0.0        # tag_blur&gt;0 blurs the image and tag_blur&lt;0 sharpens the image
tag_refine_edges:  1          # improves edge detection and therefore improves pose estimation. Light computation
tag_refine_decode: 0          # reduces false negative detection rate. Medium computation
tag_refine_pose:   0          # improves pose estimation accuracy. Heavy computation
tag_debug:         0          # save debug images to ~/.ros Careful if running with video
publish_tf:        true       # publish tag/bundle poses to /tf topic
</code></pre><h4 id=tagsyaml-parameters>tags.yaml parameters</h4><p>This file tells the algorithm what tags to look for in the environment and how large they are so they can be placed in 3D space. The software assumes distance measurements are in <code>meters</code> and orientation is in <code>quaternions</code> (One can use <code>tf.transformation</code> package for other transformations)</p><pre><code>standalone_tags:
  [
    {id: 10, size: 0.15},
    {id: 20, size: 0.1},
    {id: 30, size: 0.07}
  ]
tag_bundles:
  [
    {
      name: 'my_bundle',
      layout:
        [
          {id: 0, size: 0.05, x: 0.0000, y: 0.0000, z: 0.0, qw: 1.0, qx: 0.0, qy: 0.0, qz: 0.0},
          {id: 4, size: 0.05, x: 0.0548, y: -0.0522, z: 0.0, qw: 1.0, qx: 0.0, qy: 0.0, qz: 0.0},
          {id: 3, size: 0.05, x: -0.0580, y: -0.0553, z: 0.0, qw: 1.0, qx: 0.0, qy: 0.0, qz: 0.0},
          {id: 2, size: 0.05, x: 0.0543, y: 0.0603, z: 0.0, qw: 1.0, qx: 0.0, qy: 0.0, qz: 0.0},
          {id: 1, size: 0.05, x: -0.0582, y: 0.0573, z: 0.0, qw: 1.0, qx: 0.0, qy: 0.0, qz: 0.0}
        ]
    }
  ]
</code></pre><p>As we can see in the above configuration, We can define 2 tracking methods, <code>standalone_tags</code> and <code>tag_bundles</code>. For <code>standalone</code> tags, i.e. each marker represents a unique object in the environment we provide an ID and size for each tag you want to detect. For <code>tag_bundles</code> is the new feature which let you track an object which represented by multiple tags to overcome occlusion and such. Upon detection of a single or multiple tags in the bundle, the software will report the 6 DOF pose of the bundle&rsquo;s origin. When we create a bundle you specify a list of tags. Each tag has a tag ID, size, and 6 DOF location of the tag in reference to the bundle&rsquo;s origin.</p><p>Important notes mentioned by the authors:</p><ul><li>No tag ID should appear twice with different sizes (this creates ambiguity in the detection)</li><li>No tag ID should appear twice in the image (this creates ambiguity in the detection)</li><li>It is fine for a tag with the same ID to be listed both in standalone_tags and in tag_bundles, as long as it has the same size.</li></ul><p>A complete <code>launch</code> file which:</p><ul><li>Open the camera stream from the <code>video_device</code> with <code>width</code> and <code>height</code> parameters.</li><li>Launch the <code>image_proc</code> node to remove distortions.</li><li>Load the <code>settings.yaml</code> and <code>tags.yaml</code> configurations files to the parameters server.</li><li>Run the <code>apriltag_ros</code> core node and publish the detected transformation.</li></ul><pre><code>&lt;launch&gt;

  &lt;node name=&quot;camera&quot; pkg=&quot;usb_cam&quot; type=&quot;usb_cam_node&quot; output=&quot;screen&quot; &gt;
    &lt;param name=&quot;video_device&quot; value=&quot;/dev/video4&quot; /&gt;
    &lt;param name=&quot;image_width&quot; value=&quot;1280&quot; /&gt;
    &lt;param name=&quot;image_height&quot; value=&quot;720&quot; /&gt;
    &lt;param name=&quot;pixel_format&quot; value=&quot;yuyv&quot; /&gt;
    &lt;param name=&quot;camera_frame_id&quot; value=&quot;rgb_cam_link&quot; /&gt;
  &lt;/node&gt;

  &lt;node pkg=&quot;image_proc&quot; type=&quot;image_proc&quot; name=&quot;iamge_proc_node&quot; ns=&quot;camera&quot; /&gt;

  &lt;arg name=&quot;launch_prefix&quot; default=&quot;&quot; /&gt; &lt;!-- set to value=&quot;gdbserver localhost:10000&quot; for remote debugging --&gt;
  &lt;arg name=&quot;node_namespace&quot; default=&quot;apriltag_ros_continuous_node&quot; /&gt;
  &lt;arg name=&quot;camera_name&quot; default=&quot;/camera&quot; /&gt;
  &lt;arg name=&quot;camera_frame&quot; default=&quot;camera&quot; /&gt;
  &lt;arg name=&quot;image_topic&quot; default=&quot;image_rect&quot; /&gt;

  &lt;!-- Set parameters --&gt;
  &lt;rosparam command=&quot;load&quot; file=&quot;$(find apriltag_ros)/config/settings.yaml&quot; ns=&quot;$(arg node_namespace)&quot; /&gt;
  &lt;rosparam command=&quot;load&quot; file=&quot;$(find apriltag_ros)/config/tags.yaml&quot; ns=&quot;$(arg node_namespace)&quot; /&gt;

  &lt;node pkg=&quot;apriltag_ros&quot; type=&quot;apriltag_ros_continuous_node&quot; name=&quot;$(arg node_namespace)&quot; clear_params=&quot;true&quot; output=&quot;screen&quot; launch-prefix=&quot;$(arg launch_prefix)&quot; &gt;
    &lt;!-- Remap topics from those used in code to those on the ROS network --&gt;
    &lt;remap from=&quot;image_rect&quot; to=&quot;$(arg camera_name)/$(arg image_topic)&quot; /&gt;
    &lt;remap from=&quot;camera_info&quot; to=&quot;$(arg camera_name)/camera_info&quot; /&gt;

    &lt;param name=&quot;camera_frame&quot; type=&quot;str&quot; value=&quot;$(arg camera_frame)&quot; /&gt;
    &lt;param name=&quot;publish_tag_detections_image&quot; type=&quot;bool&quot; value=&quot;true&quot; /&gt;      &lt;!-- default: false --&gt;
  &lt;/node&gt;

&lt;/launch&gt;
</code></pre></p></details><details class=spoiler id=spoiler-2><summary>ArTag Markers</summary><p><p>In this part ill explain the usage of <code>ar_track_alvar</code> package for AR tag tracking. This package is a ROS wrapper for <a href=http://virtual.vtt.fi/virtual/proj2/multimedia/index.html>Alvar</a>, an open source AR tag tracking library.</p><p align=center><img src=./OpenManipulator_AR_Marker.png width=80%></p><p><code>ar_track_alvar</code> node has 4 main functionalities:</p><ul><li>Generating AR tags of varying size, resolution, and data/ID encoding</li><li>Identifying and tracking the pose of individual AR tags, optionally integrating kinect depth data (when a kinect is available) for better pose estimates.
Identifying and tracking the pose of <code>bundles</code> consisting of multiple tags. This allows for more stable pose estimates, robustness to occlusions, and tracking of multi-sided objects.</li></ul><p>Alvar features adaptive thresholding to handle a variety of lighting conditions, optical flow based tracking for more stable pose estimation, and an improved tag identification method that does not significantly slow down as the number of tags increases.</p><h4 id=installation>Installation</h4><p>for melodic-devel:</p><pre><code>sudo apt-get install ros-melodic-ar-track-alvar ros-melodic-ar-track-alvar-msgs ros-melodic-image-proc
</code></pre><p>or just clone and build the github repo <a href=https://github.com/ros-perception/ar_track_alvar>Github</a>.</p><p>In order to use AR Marker properly with your camera, be sure to add the camera model to the launch command when using AR Marker (camera_calibration process as described in aprtiltag section.</p><h4 id=generating-ar-tags>Generating AR tags</h4><p>Two pdf files are in the markers directory containing tags 0-8 and 9-17, respectively. Alternativly, we can get the tags from <a href=http://wiki.ros.org/ar_track_alvar>ar_track_alvar</a> and resize to our use. If you want to generate your own markers with different ID numbers, border widths, or sizes, run:</p><pre><code>rosrun ar_track_alvar createMarker 0 -s 10.0

</code></pre><p>This will create <code>MarkerData_0.png</code> file that stores a 10cm x 10cm marker with id 0. Print this file on a sheet of paper.</p><p>Due to differences in printer setups, the actual size of the printed marker may be different. Make sure the <code>marker_size</code> parameter represents the actual size (in centimeters) of the AR tag.</p><h4 id=tracking>Tracking</h4><p>In order to identify and track the poses of (possibly) multiple AR tags that are each considered individually. The packges uses the node <code>individualMarkers</code> which takes the following parameters:</p><ul><li><code>marker_size</code> (double) &ndash; The width in centimeters of one side of the black square marker border</li><li><code>max_new_marker_error</code> (double) &ndash; A threshold determining when new markers can be detected under uncertainty</li><li><code>max_track_error</code> (double) &ndash; A threshold determining how much tracking error can be observed before an tag is considered to have disappeared</li><li><code>camera_image</code> (string) &ndash; The name of the topic that provides camera frames for detecting the AR tags. This can be mono or color, but should be an UNrectified image, since rectification takes place in this package</li><li><code>camera_info</code> (string) &ndash; The name of the topic that provides the camera calibration parameters so that the image can be rectified</li><li><code>output_frame</code> (string) &ndash; The name of the frame that the published Cartesian locations of the AR tags will be relative to</li></ul><p><strong>IMPORTANT:</strong> The node assumes that a Kinect camera being used as the camera, so that depth data can be integrated for better pose estimates. If you are not using a Kinect or do not desire to use depth data improvements, use <code>individualMarkersNoKinect</code> instead.</p><p>To use the package we need to create a new <code>launch</code> file. Inside <code>launch/</code> directory add <code>alvar_track.launch</code> with the following content:</p><pre><code>&lt;launch&gt;

&lt;arg name=&quot;marker_frame_id&quot;     default=&quot;world&quot;/&gt;
&lt;arg name=&quot;user_marker_size&quot;	  default=&quot;7.0&quot;/&gt;

&lt;arg name=&quot;camera_model&quot; default=&quot;astra_pro&quot; doc=&quot;model type [astra_pro, realsense_d435, raspicam]&quot;/&gt;
&lt;arg name=&quot;camera_namespace&quot; default=&quot;camera&quot;/&gt;
&lt;arg name=&quot;rgb_camera_info_url&quot;   default=&quot;package://open_manipulator_p_camera/camera_info/$(arg camera_model).yaml&quot; /&gt;
&lt;arg name=&quot;depth_camera_info_url&quot; default=&quot;&quot; /&gt;

&lt;include file=&quot;$(find ar_track_alvar)/launch/pr2_indiv_no_kinect.launch&quot;&gt;
  &lt;arg name=&quot;marker_size&quot; value=&quot;$(arg user_marker_size)&quot; /&gt;
  &lt;arg name=&quot;max_new_marker_error&quot; value=&quot;0.08&quot; /&gt;
  &lt;arg name=&quot;max_track_error&quot; value=&quot;0.2&quot; /&gt;
  &lt;arg name=&quot;cam_image_topic&quot; value=&quot;$(arg camera_namespace)/image_raw&quot; /&gt;
  &lt;arg name=&quot;cam_info_topic&quot; value=&quot;$(arg camera_namespace)/camera_info&quot; /&gt;
  &lt;arg name=&quot;output_frame&quot; value=&quot;$(arg marker_frame_id)&quot; /&gt;
&lt;/include&gt;

&lt;/launch&gt;
</code></pre><h4 id=bundle>Bundle</h4><p>Sometimes it is advantageous to treat &ldquo;bundles&rdquo; of multiple tags as a single unit. For example, this can allow for the estimation of the pose of a many-sided object, even when some of the tags cannot be seen. A tag bundle is defined by an XML file that lists a set of tag IDs and their positions relative to a <code>master</code> tag. The master tag always comes first in the XML file and defines a coordinate system for the rest of the tags.</p><p><strong>IMPORTANT</strong>: this coordinate system is different from the standard system used in ROS! In this system, when facing the tag, positive-z comes out of the front of the tag toward the viewer, positive-x is to the right, and positive-y is up.</p><p>To create a bundle, first choose which tag you want to be the master tag. Treat the center of the master tag as (0,0,0). Then, after placing the rest of the tags, measure the x, y, and z coordinate for each of the 4 corners of all of the tags, relative to the master tag origin. Enter these measurements for each tag into the XML file starting with the lower left corner and progressing counter-clockwise around the tag. After creating the XML file all you need is to add the following parameter to your launch file.</p><ul><li><code>bundle_files</code> (multiple strings) &ndash; A list of XML file names, one for each bundle you wish to detect.</li></ul><p>An example XML file for 2 markers:</p><pre><code>&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot; standalone=&quot;no&quot; ?&gt;
&lt;multimarker markers=&quot;2&quot;&gt;
    &lt;marker index=&quot;8&quot; status=&quot;1&quot;&gt;
        &lt;corner x=&quot;-2.2&quot; y=&quot;-2.2&quot; z=&quot;0.0&quot; /&gt;
        &lt;corner x=&quot;2.2&quot; y=&quot;-2.2&quot; z=&quot;0.0&quot; /&gt;
        &lt;corner x=&quot;2.2&quot; y=&quot;2.2&quot; z=&quot;0.0&quot; /&gt;
        &lt;corner x=&quot;-2.2&quot; y=&quot;2.2&quot; z=&quot;0.0&quot; /&gt;
    &lt;/marker&gt;
    &lt;marker index=&quot;9&quot; status=&quot;1&quot;&gt;
        &lt;corner x=&quot;-2.2&quot; y=&quot;11.8&quot; z=&quot;0.0&quot; /&gt;
        &lt;corner x=&quot;2.2&quot; y=&quot;11.8&quot; z=&quot;0.0&quot; /&gt;
        &lt;corner x=&quot;2.2&quot; y=&quot;16.2&quot; z=&quot;0.0&quot; /&gt;
        &lt;corner x=&quot;-2.2&quot; y=&quot;16.2&quot; z=&quot;0.0&quot; /&gt;
    &lt;/marker&gt;
&lt;/multimarker&gt;
</code></pre></p></details><details class=spoiler id=spoiler-3><summary>ArUco Marker</summary><p>Todo.</p></details><details class=spoiler id=spoiler-4><summary>OptiTrack</summary><p>Todo.</p></details></div><div class=share-box aria-hidden=true><ul class=share><li><a href="https://twitter.com/intent/tweet?url=https://osheraz.github.io/blog/track/&text=Pose%20Estimation%20using%20Markers" target=_blank rel=noopener class=share-btn-twitter><i class="fab fa-twitter"></i></a></li><li><a href="https://www.facebook.com/sharer.php?u=https://osheraz.github.io/blog/track/&t=Pose%20Estimation%20using%20Markers" target=_blank rel=noopener class=share-btn-facebook><i class="fab fa-facebook"></i></a></li><li><a href="mailto:?subject=Pose%20Estimation%20using%20Markers&body=https://osheraz.github.io/blog/track/" target=_blank rel=noopener class=share-btn-email><i class="fas fa-envelope"></i></a></li><li><a href="https://www.linkedin.com/shareArticle?url=https://osheraz.github.io/blog/track/&title=Pose%20Estimation%20using%20Markers" target=_blank rel=noopener class=share-btn-linkedin><i class="fab fa-linkedin-in"></i></a></li><li><a href="whatsapp://send?text=Pose%20Estimation%20using%20Markers%20https://osheraz.github.io/blog/track/" target=_blank rel=noopener class=share-btn-whatsapp><i class="fab fa-whatsapp"></i></a></li><li><a href="https://service.weibo.com/share/share.php?url=https://osheraz.github.io/blog/track/&title=Pose%20Estimation%20using%20Markers" target=_blank rel=noopener class=share-btn-weibo><i class="fab fa-weibo"></i></a></li></ul></div><div class="media author-card content-widget-hr"><a href=https://osheraz.github.io/><img class="avatar mr-3 avatar-circle" src=/authors/osher-azulay/avatar_hub134170bfb8ac65f2ae4f5f140cab5f2_67039_270x270_fill_q75_lanczos_center.jpg alt="Osher Azulay"></a><div class=media-body><h5 class=card-title><a href=https://osheraz.github.io/>Osher Azulay</a></h5><h6 class=card-subtitle>Roboticist</h6><p class=card-text>My research interests include robotic manipulation, deep reinforcement learning.</p><ul class=network-icon aria-hidden=true><li><a href=/#contact><i class="fas fa-envelope"></i></a></li><li><a href=https://github.com/osheraz target=_blank rel=noopener><i class="fab fa-github"></i></a></li><li><a href=https://www.linkedin.com/in/osher-azulay-20ab38154/ target=_blank rel=noopener><i class="fab fa-linkedin"></i></a></li></ul></div></div></div></article></div><div class=page-footer><div class=container><footer class=site-footer><p class=powered-by>Published with <a href="https://wowchemy.com/?utm_campaign=poweredby" target=_blank rel=noopener>Wowchemy</a> — the free, <a href=https://github.com/wowchemy/wowchemy-hugo-modules target=_blank rel=noopener>open source</a> website builder that empowers creators.</p></footer></div></div><div id=modal class="modal fade" role=dialog><div class=modal-dialog><div class=modal-content><div class=modal-header><h5 class=modal-title>Cite</h5><button type=button class=close data-dismiss=modal aria-label=Close>
<span aria-hidden=true>&#215;</span></button></div><div class=modal-body><pre><code class="tex hljs"></code></pre></div><div class=modal-footer><a class="btn btn-outline-primary my-1 js-copy-cite" href=# target=_blank><i class="fas fa-copy"></i> Copy</a>
<a class="btn btn-outline-primary my-1 js-download-cite" href=# target=_blank><i class="fas fa-download"></i> Download</a><div id=modal-error></div></div></div></div></div><script src=https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js integrity="sha256-9/aliU8dGd2tb6OSsuzixeV4y/faTqgFtohetphbbj0=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/instant.page/5.1.0/instantpage.min.js integrity="sha512-1+qUtKoh9XZW7j+6LhRMAyOrgSQKenQ4mluTR+cvxXjP1Z54RxZuzstR/H9kgPXQsVB8IW7DMDFUJpzLjvhGSQ==" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.4/imagesloaded.pkgd.min.js integrity="sha256-lqvxZrPLtfffUl2G/e7szqSvPBILGbwmsGE1MKlOi0Q=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.6/isotope.pkgd.min.js integrity="sha256-CBrpuqrMhXwcLLUd5tvQ4euBHCdh7wGlDfNz8vbu/iI=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/highlight.min.js integrity="sha512-TDKKr+IvoqZnPzc3l35hdjpHD0m+b2EC2SrLEgKDRWpxf2rFCxemkgvJ5kfU48ip+Y+m2XVKyOCD85ybtlZDmw==" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/languages/r.min.js></script><script id=search-hit-fuse-template type=text/x-template>
        <div class="search-hit" id="summary-{{key}}">
          <div class="search-hit-content">
            <div class="search-hit-name">
              <a href="{{relpermalink}}">{{title}}</a>
              <div class="article-metadata search-hit-type">{{type}}</div>
              <p class="search-hit-description">{{snippet}}</p>
            </div>
          </div>
        </div>
      </script><script src=https://cdnjs.cloudflare.com/ajax/libs/fuse.js/3.2.1/fuse.min.js integrity="sha256-VzgmKYmhsGNNN4Ph1kMW+BjoYJM2jV5i4IlFoeZA9XI=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/jquery.mark.min.js integrity="sha256-4HLtjeVgH0eIB3aZ9mLYF6E8oU5chNdjU6p6rrXpl9U=" crossorigin=anonymous></script><script src=/js/bootstrap.bundle.min.6aed84840afc03ab4d5750157f69c120.js></script><script src=/en/js/wowchemy.min.8626d678bd286b6b2b19961df891e128.js></script></body></html>